{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29076a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark= SparkSession.builder\\\n",
    "    .appName('spark-pyspark-databrick-pr')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8faf92ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://kubernetes.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark-pyspark-databrick-pr</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2155c6a8c70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8a27209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+------+---------+\n",
      "|order_id|customer_id|order_date|amount|     city|\n",
      "+--------+-----------+----------+------+---------+\n",
      "|       1|        101|2023-01-15|250.75|    Delhi|\n",
      "|       2|        102|2023-01-16| 100.0|   Mumbai|\n",
      "|       3|        103|2023-01-17| 500.0|Bangalore|\n",
      "|       4|        101|2023-02-05|700.25|    Delhi|\n",
      "|       5|        104|2023-02-15| 200.0|  Chennai|\n",
      "|       6|        105|2023-03-10| 150.0|  Kolkata|\n",
      "|       7|        102|2023-03-12| 300.5|   Mumbai|\n",
      "|       8|        106|2023-03-15| 800.0|Hyderabad|\n",
      "+--------+-----------+----------+------+---------+\n",
      "\n",
      "+-----------+-------+------+----------+-------------------+\n",
      "|customer_id|   name|gender|birth_date|              email|\n",
      "+-----------+-------+------+----------+-------------------+\n",
      "|        101|  Alice|     F|1990-06-10|  alice@example.com|\n",
      "|        102|    Bob|     M|1985-09-21|    bob@example.com|\n",
      "|        103|Charlie|     M|1992-01-15|charlie@example.com|\n",
      "|        104|  Diana|     F|1988-12-05|  diana@example.com|\n",
      "|        105|   Evan|     M|1995-04-17|   evan@example.com|\n",
      "|        107|  Grace|     F|1993-11-29|  grace@example.com|\n",
      "+-----------+-------+------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "orders_data = [\n",
    "    (1, 101, '2023-01-15', 250.75, 'Delhi'),\n",
    "    (2, 102, '2023-01-16', 100.00, 'Mumbai'),\n",
    "    (3, 103, '2023-01-17', 500.00, 'Bangalore'),\n",
    "    (4, 101, '2023-02-05', 700.25, 'Delhi'),\n",
    "    (5, 104, '2023-02-15', 200.00, 'Chennai'),\n",
    "    (6, 105, '2023-03-10', 150.00, 'Kolkata'),\n",
    "    (7, 102, '2023-03-12', 300.50, 'Mumbai'),\n",
    "    (8, 106, '2023-03-15', 800.00, 'Hyderabad')\n",
    "]\n",
    "\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"order_date\", StringType(), True),\n",
    "    StructField(\"amount\", FloatType(), True),\n",
    "    StructField(\"city\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_order=spark.createDataFrame(data=orders_data, schema=orders_schema)\n",
    "df_torder=df_order.withColumn(\"order_date\", to_date(col(\"order_date\")))\n",
    "df_torder.show()\n",
    "\n",
    "customers_data = [\n",
    "    (101, \"Alice\", \"F\", \"1990-06-10\", \"alice@example.com\"),\n",
    "    (102, \"Bob\", \"M\", \"1985-09-21\", \"bob@example.com\"),\n",
    "    (103, \"Charlie\", \"M\", \"1992-01-15\", \"charlie@example.com\"),\n",
    "    (104, \"Diana\", \"F\", \"1988-12-05\", \"diana@example.com\"),\n",
    "    (105, \"Evan\", \"M\", \"1995-04-17\", \"evan@example.com\"),\n",
    "    (107, \"Grace\", \"F\", \"1993-11-29\", \"grace@example.com\")\n",
    "]\n",
    "\n",
    "\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"birth_date\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_customer=spark.createDataFrame(customers_data, customers_schema)\n",
    "df_tcustomer=df_customer.withColumn(\"birth_date\", df_customer[\"birth_date\"].cast(\"date\"))\n",
    "df_tcustomer.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
