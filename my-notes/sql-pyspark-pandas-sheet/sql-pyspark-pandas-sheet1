-------- mysql table and data create ------------

-- Create the Person table
CREATE TABLE Person (
    personId INT PRIMARY KEY,
    lastName VARCHAR(255),
    firstName VARCHAR(255)
);

-- Create the Address table
CREATE TABLE Address (
    addressId INT PRIMARY KEY,
    personId INT,
    city VARCHAR(255),
    state VARCHAR(255),
    FOREIGN KEY (personId) REFERENCES Person(personId)
);

INSERT INTO Person (personId, lastName, firstName) VALUES
(1, 'Smith', 'John'),
(2, 'Johnson', NULL),
(3, NULL, 'Liam'),
(4, 'Taylor', 'Olivia'),
(5, 'Anderson', NULL),
(6, NULL, 'Ava'),
(7, 'Jackson', 'William'),
(8, 'White', 'Sophia'),
(9, 'Harris', NULL),
(10, NULL, 'Isabella'),
(11, 'Clark', 'Emma'),
(12, 'Martinez', 'Mia'),
(13, NULL, 'Ethan'),
(14, 'Lee', 'James'),
(15, 'Walker', NULL);

INSERT INTO Address (addressId, personId, city, state) VALUES
(1, 1, 'New York', 'NY'),
(2, 2, NULL, 'CA'),
(3, NULL, 'Chicago', 'IL'), 
(4, 4, 'Houston', 'TX'),
(5, NULL, 'Phoenix', 'AZ'), 
(6, 6, NULL, 'PA'),
(7, NULL, 'San Antonio', 'TX'), 
(8, 8, 'San Diego', NULL),
(9, 9, 'Dallas', 'TX'),
(10, 10, 'San Jose', NULL),
(11, 11, 'Austin', 'TX'),
(12, NULL, 'Boston', 'MA'), 
(13, 13, NULL, 'FL'),
(14, 14, 'Seattle', 'WA'),
(15, NULL, 'Denver', NULL), 
(16, 1, 'Brooklyn', 'NY'),
(17, 2, 'Los Angeles', 'CA'), 
(18, 8, 'La Jolla', 'CA'), 
(19, 11, 'Round Rock', 'TX'), 
(20, 4, 'Dallas', 'TX'); 

------------------------ pyspark data and schema ----------------

# Define schema for Person table
person_schema = StructType([
    StructField("personId", IntegerType(), True),
    StructField("lastName", StringType(), True),
    StructField("firstName", StringType(), True)
])

# Define schema for Address table
address_schema = StructType([
    StructField("addressId", IntegerType(), True),
    StructField("personId", IntegerType(), True),
    StructField("city", StringType(), True),
    StructField("state", StringType(), True)
])

person_data = [
    (1, 'Smith', 'John'),
    (2, 'Johnson', None),
    (3, None, 'Liam'),
    (4, 'Taylor', 'Olivia'),
    (5, 'Anderson', None),
    (6, None, 'Ava'),
    (7, 'Jackson', 'William'),
    (8, 'White', 'Sophia'),
    (9, 'Harris', None),
    (10, None, 'Isabella'),
    (11, 'Clark', 'Emma'),
    (12, 'Martinez', 'Mia'),
    (13, None, 'Ethan'),
    (14, 'Lee', 'James'),
    (15, 'Walker', None)
]

address_data = [
    (1, 1, 'New York', 'NY'),
    (2, 2, None, 'CA'),
    (3, None, 'Chicago', 'IL'),  
    (4, 4, 'Houston', 'TX'),
    (5, None, 'Phoenix', 'AZ'),  
    (6, 6, None, 'PA'),
    (7, None, 'San Antonio', 'TX'),  
    (8, 8, 'San Diego', None),
    (9, 9, 'Dallas', 'TX'),
    (10, None, 'San Jose', None),  
    (11, 11, 'Austin', 'TX'),
    (12, None, 'Boston', 'MA'),  
    (13, 13, None, 'FL'),
    (14, 14, 'Seattle', 'WA'),
    (15, None, 'Denver', None),  
    (16, 1, 'Brooklyn', 'NY'),  
    (17, 2, 'Los Angeles', 'CA'),  
    (18, 8, 'La Jolla', 'CA'),  
    (19, 11, 'Round Rock', 'TX'),  
    (20, 4, 'Dallas', 'TX')  
]


----------------- pandas schema and dataset -----------

person_columns = ['personId', 'lastName', 'firstName']
address_columns = ['addressId', 'personId', 'city', 'state']

person_df = person_df.astype({'personId': 'int64', 'lastName': 'str', 'firstName': 'str'})
address_df = address_df.astype({'addressId': 'int64', 'personId': 'int64', 'city': 'str', 'state': 'str'})


person_data = [
    (1, 'Smith', 'John'),
    (2, 'Johnson', None),
    (3, None, 'Liam'),
    (4, 'Taylor', 'Olivia'),
    (5, 'Anderson', None),
    (6, None, 'Ava'),
    (7, 'Jackson', 'William'),
    (8, 'White', 'Sophia'),
    (9, 'Harris', None),
    (10, None, 'Isabella'),
    (11, 'Clark', 'Emma'),
    (12, 'Martinez', 'Mia'),
    (13, None, 'Ethan'),
    (14, 'Lee', 'James'),
    (15, 'Walker', None)
]


address_data = [
    (1, 1, 'New York', 'NY'),
    (2, 2, None, 'CA'),
    (3, None, 'Chicago', 'IL'),  # Missing personId
    (4, 4, 'Houston', 'TX'),
    (5, None, 'Phoenix', 'AZ'),  # Missing personId
    (6, 6, None, 'PA'),
    (7, None, 'San Antonio', 'TX'),  # Missing personId
    (8, 8, 'San Diego', None),
    (9, 9, 'Dallas', 'TX'),
    (10, None, 'San Jose', None),  # Missing personId
    (11, 11, 'Austin', 'TX'),
    (12, None, 'Boston', 'MA'),  # Missing personId
    (13, 13, None, 'FL'),
    (14, 14, 'Seattle', 'WA'),
    (15, None, 'Denver', None),  # Missing personId
    (16, 1, 'Brooklyn', 'NY'),  # Duplicate personId for personId = 1
    (17, 2, 'Los Angeles', 'CA'),  # Duplicate personId for personId = 2
    (18, 8, 'La Jolla', 'CA'),  # Duplicate personId for personId = 8
    (19, 11, 'Round Rock', 'TX'),  # Duplicate personId for personId = 11
    (20, 4, 'Dallas', 'TX')  # Duplicate personId for personId = 4
]


----------------- practice questions -----------------
Practice Questions

#Basic Data Inspection

Pandas:
-- Display the first 5 rows of each DataFrame.
-- Count the total number of rows in each DataFrame.
-- Display the data types of each column in both DataFrames.

PySpark:
-- Display the first 5 rows of each DataFrame.
-- Count the total number of rows in each DataFrame.
-- Print the schema of both DataFrames.

MySQL:
-- Select the first 5 rows from each table.
-- Count the total number of rows in each table.
-- Show the structure of both tables using DESCRIBE.

#Filtering Data

Pandas:
-- Find all persons whose lastName is NULL (use .isnull()).
-- Filter the Address DataFrame for rows where the state is TX.
-- Select rows where firstName starts with the letter J (use .str.startswith()).

PySpark:
-- Find all persons whose lastName is NULL.
-- Find all addresses located in the state of TX.
-- Get all persons whose firstName starts with the letter J.

MySQL:
-- Select all rows from the Person table where lastName is NULL.
-- Find all addresses in the Address table where state is 'TX'.
-- Retrieve all persons whose firstName starts with 'J'.

#Joining Tables

Pandas:
-- Perform an inner join between the Person and Address DataFrames on personId (use .merge()).
-- Perform a left join to get all persons and their addresses, even if the address is NULL.
-- Find persons who have no address in the Address DataFrame.

PySpark:
-- Perform an inner join between the Person and Address DataFrames on personId.
-- Perform a left join to get all persons and their addresses, even if the address is NULL.
-- Find persons who have no address in the Address table.

MySQL:
-- Write a query to perform an inner join between Person and Address on personId.
-- Perform a left join to list all persons and their addresses, including NULL values for missing addresses.
-- Find all persons who donâ€™t have an address in the Address table.

#Aggregation

Pandas:
-- Count the number of addresses in each state (use .groupby() and .size()).
-- Find the total number of unique city values in the Address DataFrame.
-- Count how many persons have a NULL value for firstName.

PySpark:
-- Count the number of addresses in each state.
-- Find the total number of unique city values in the Address DataFrame.
-- Count how many persons have a NULL value for firstName.

MySQL:
-- Count the number of addresses grouped by state.
-- Find the total number of unique city values in the Address table.
-- Count the number of persons with a NULL value for firstName.

#Data Transformation

Pandas:
-- Create a new column fullName in the Person DataFrame by combining firstName and lastName.
-- Replace NULL values in the city column of the Address DataFrame with 'Unknown'.
-- Extract only the personId and city columns from the Address DataFrame.

PySpark:
-- Add a new column to the Person DataFrame called fullName that combines firstName and lastName.
-- Replace NULL values in the city column of the Address DataFrame with 'Unknown'.
-- Create a new DataFrame that contains only the personId and city columns from the Address DataFrame.

MySQL:
-- Use a query to create a new column fullName by concatenating firstName and lastName in the Person table.
-- Update the Address table to replace NULL values in the city column with 'Unknown'.
-- Select only the personId and city columns from the Address table.

#Advanced Queries

Pandas:
-- Find all cities that have more than one address associated with them.
-- Create a DataFrame of persons who live in cities starting with the letter S.
-- Determine the number of unique states in the Address DataFrame.

PySpark:
-- Find all cities that have more than one address associated with them.
-- Create a new DataFrame with persons who live in a city that starts with the letter S.
-- Determine the number of unique states represented in the Address DataFrame.

MySQL:
-- Write a query to find cities with more than one address in the Address table.
-- Select all persons who live in cities that start with 'S'.
-- Count the number of distinct states in the Address table.

#Handling Missing Data

Pandas:
-- Drop rows in the Person DataFrame where firstName is NULL (use .dropna()).
-- Fill NULL values in the lastName column with 'Unknown'.
-- Replace all NULL values in the Address DataFrame with default values.

PySpark:
-- Drop rows in the Person DataFrame where firstName is NULL.
-- Fill NULL values in the lastName column with 'Unknown'.
-- Replace all NULL values in the Address DataFrame with default values.

MySQL:
-- Delete rows from the Person table where firstName is NULL.
-- Update lastName to 'Unknown' where it is NULL in the Person table.
-- Update all NULL values in the Address table with default values.

#Exporting and Saving Data

Pandas:
-- Save the Person DataFrame to a CSV file using .to_csv().
-- Save the Address DataFrame to an Excel file using .to_excel().
-- Save the result of a join operation between Person and Address to a JSON file using .to_json().

PySpark:
-- Save the Person DataFrame as a CSV file.
-- Write the Address DataFrame to a Parquet file.
-- Save the result of an inner join between Person and Address as a JSON file.

MySQL:
-- Write a query to export the Person table to a CSV file.
-- Export the Address table to a JSON file.
-- Save the result of an inner join query to a new table called PersonAddress.

#Challenges
-- Pandas: Identify and drop duplicate rows in the Person DataFrame.
-- PySpark: Identify if there are duplicate rows in the Person DataFrame and remove them.
-- MySQL: Write a query to find all persons who have addresses in multiple states.
-- Pandas/PySpark: Write a script to load the Person and Address tables from CSV files, perform an inner join, and save the result as a new Parquet file.