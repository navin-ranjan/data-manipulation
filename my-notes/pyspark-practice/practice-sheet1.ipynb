{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import class of project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkSession start write pyspark code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.driver.host', 'DESKTOP-OI3A44I'), ('spark.app.name', 'practice-sheet1'), ('spark.rdd.compress', 'True'), ('spark.app.startTime', '1737478978144'), ('spark.master', 'local[*]'), ('spark.app.submitTime', '1737478977816'), ('spark.driver.port', '55479'), ('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.executor.id', 'driver'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.app.id', 'local-1737478979822'), ('spark.serializer.objectStreamReset', '100'), ('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.ui.showConsoleProgress', 'true')]\n"
     ]
    }
   ],
   "source": [
    "spark=SparkSession.builder.appName(\"practice-sheet1\").getOrCreate();\n",
    "print(spark.sparkContext.getConf().getAll())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create schema of Person and Address "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('personId', IntegerType(), True), StructField('lastName', StringType(), True), StructField('firstName', StringType(), True)]) StructType([StructField('addressId', IntegerType(), True), StructField('personId', IntegerType(), True), StructField('city', StringType(), True), StructField('state', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType,StructField, IntegerType, StringType\n",
    "#person schema\n",
    "person_schema=StructType([\n",
    "    StructField(\"personId\", IntegerType(), True),\n",
    "    StructField(\"lastName\", StringType(), True),\n",
    "    StructField(\"firstName\", StringType(), True)                     \n",
    "])\n",
    "\n",
    "#Address schema\n",
    "\n",
    "address_schema=StructType([\n",
    "    StructField(\"addressId\", IntegerType(), True),\n",
    "    StructField(\"personId\", IntegerType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True)\n",
    "])\n",
    "\n",
    "print(person_schema, address_schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Data of person and addrees Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- personId: integer (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      "\n",
      "+--------+--------+---------+\n",
      "|personId|lastName|firstName|\n",
      "+--------+--------+---------+\n",
      "|       1|   Smith|     John|\n",
      "|       2| Johnson|     null|\n",
      "+--------+--------+---------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- addressId: integer (nullable = true)\n",
      " |-- personId: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n",
      "+---------+--------+--------+-----+\n",
      "|addressId|personId|    city|state|\n",
      "+---------+--------+--------+-----+\n",
      "|        1|       1|New York|   NY|\n",
      "|        2|       2|    null|   CA|\n",
      "+---------+--------+--------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person_data = [\n",
    "    (1, 'Smith', 'John'),\n",
    "    (2, 'Johnson', None),\n",
    "    (3, None, 'Liam'),\n",
    "    (4, 'Taylor', 'Olivia'),\n",
    "    (5, 'Anderson', None),\n",
    "    (6, None, 'Ava'),\n",
    "    (7, 'Jackson', 'William'),\n",
    "    (8, 'White', 'Sophia'),\n",
    "    (9, 'Harris', None),\n",
    "    (10, None, 'Isabella'),\n",
    "    (11, 'Clark', 'Emma'),\n",
    "    (12, 'Martinez', 'Mia'),\n",
    "    (13, None, 'Ethan'),\n",
    "    (14, 'Lee', 'James'),\n",
    "    (15, 'Walker', None)\n",
    "]\n",
    "\n",
    "\n",
    "address_data = [\n",
    "    (1, 1, 'New York', 'NY'),\n",
    "    (2, 2, None, 'CA'),\n",
    "    (3, None, 'Chicago', 'IL'),  \n",
    "    (4, 4, 'Houston', 'TX'),\n",
    "    (5, None, 'Phoenix', 'AZ'),  \n",
    "    (6, 6, None, 'PA'),\n",
    "    (7, None, 'San Antonio', 'TX'),  \n",
    "    (8, 8, 'San Diego', None),\n",
    "    (9, 9, 'Dallas', 'TX'),\n",
    "    (10, None, 'San Jose', None),  \n",
    "    (11, 11, 'Austin', 'TX'),\n",
    "    (12, None, 'Boston', 'MA'),  \n",
    "    (13, 13, None, 'FL'),\n",
    "    (14, 14, 'Seattle', 'WA'),\n",
    "    (15, None, 'Denver', None),  \n",
    "    (16, 1, 'Brooklyn', 'NY'),  \n",
    "    (17, 2, 'Los Angeles', 'CA'),  \n",
    "    (18, 8, 'La Jolla', 'CA'),  \n",
    "    (19, 11, 'Round Rock', 'TX'),  \n",
    "    (20, 4, 'Dallas', 'TX')  \n",
    "]\n",
    "\n",
    "df_person=spark.createDataFrame(data=person_data, schema=person_schema)\n",
    "df_person.printSchema()\n",
    "df_person.show(2)\n",
    "\n",
    "df_address=spark.createDataFrame(data=address_data, schema=address_schema)\n",
    "df_address.printSchema()\n",
    "df_address.show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Display the first 5 rows of each DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'display'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf_address\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m()\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.4.3\\python\\pyspark\\sql\\dataframe.py:2979\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2946\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[0;32m   2947\u001b[0m \n\u001b[0;32m   2948\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2976\u001b[0m \u001b[38;5;124;03m+---+\u001b[39;00m\n\u001b[0;32m   2977\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2978\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m-> 2979\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   2980\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name)\n\u001b[0;32m   2981\u001b[0m     )\n\u001b[0;32m   2982\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mapply(name)\n\u001b[0;32m   2983\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'display'"
     ]
    }
   ],
   "source": [
    "df_address.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Count the total number of rows in each DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print(df_person.count())\n",
    "print(df_address.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Print the schema of both DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- personId: integer (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- addressId: integer (nullable = true)\n",
      " |-- personId: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_person.printSchema()\n",
    "df_address.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Find all persons whose lastName is NULL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+\n",
      "|personId|lastName|firstName|\n",
      "+--------+--------+---------+\n",
      "|       3|    null|     Liam|\n",
      "|       6|    null|      Ava|\n",
      "|      10|    null| Isabella|\n",
      "|      13|    null|    Ethan|\n",
      "+--------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df_p_null=df_person.filter(col(\"lastName\").isNull())\n",
    "df_p_null.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+\n",
      "|personId|lastName|firstName|\n",
      "+--------+--------+---------+\n",
      "|       2| Johnson|     null|\n",
      "+--------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_lastname_null=df_person.where(df_person.lastName == \"Johnson\")\n",
    "df_lastname_null.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Find all addresses located in the state of TX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----------+-----+\n",
      "|addressId|personId|       city|state|\n",
      "+---------+--------+-----------+-----+\n",
      "|        4|       4|    Houston|   TX|\n",
      "|        7|    null|San Antonio|   TX|\n",
      "|        9|       9|     Dallas|   TX|\n",
      "|       11|      11|     Austin|   TX|\n",
      "|       19|      11| Round Rock|   TX|\n",
      "|       20|       4|     Dallas|   TX|\n",
      "+---------+--------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_state=df_address.filter(col(\"state\") == \"TX\")\n",
    "df_state.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Get all persons whose firstName starts with the letter J."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+\n",
      "|personId|lastName|firstName|\n",
      "+--------+--------+---------+\n",
      "|       1|   Smith|     John|\n",
      "|      14|     Lee|    James|\n",
      "+--------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_first_j = df_person.where(col(\"firstName\").startswith(\"J\"));\n",
    "df_first_j.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Perform an inner join between the Person and Address DataFrames on personId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+---------+-----------+-----+\n",
      "|personId|lastName|firstName|addressId|       city|state|\n",
      "+--------+--------+---------+---------+-----------+-----+\n",
      "|       1|   Smith|     John|        1|   New York|   NY|\n",
      "|       1|   Smith|     John|       16|   Brooklyn|   NY|\n",
      "|       2| Johnson|     null|        2|       null|   CA|\n",
      "|       2| Johnson|     null|       17|Los Angeles|   CA|\n",
      "|       4|  Taylor|   Olivia|        4|    Houston|   TX|\n",
      "|       4|  Taylor|   Olivia|       20|     Dallas|   TX|\n",
      "|       6|    null|      Ava|        6|       null|   PA|\n",
      "|       8|   White|   Sophia|        8|  San Diego| null|\n",
      "|       8|   White|   Sophia|       18|   La Jolla|   CA|\n",
      "|       9|  Harris|     null|        9|     Dallas|   TX|\n",
      "|      11|   Clark|     Emma|       11|     Austin|   TX|\n",
      "|      11|   Clark|     Emma|       19| Round Rock|   TX|\n",
      "|      13|    null|    Ethan|       13|       null|   FL|\n",
      "|      14|     Lee|    James|       14|    Seattle|   WA|\n",
      "+--------+--------+---------+---------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jd_df=df_person.join(df_address, on=\"personId\", how=\"inner\")\n",
    "jd_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Perform a left join to get all persons and their addresses, even if the address is NULL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+---------+-----------+-----+\n",
      "|personId|lastName|firstName|addressId|       city|state|\n",
      "+--------+--------+---------+---------+-----------+-----+\n",
      "|       1|   Smith|     John|       16|   Brooklyn|   NY|\n",
      "|       1|   Smith|     John|        1|   New York|   NY|\n",
      "|       3|    null|     Liam|     null|       null| null|\n",
      "|       2| Johnson|     null|       17|Los Angeles|   CA|\n",
      "|       2| Johnson|     null|        2|       null|   CA|\n",
      "|       5|Anderson|     null|     null|       null| null|\n",
      "|       4|  Taylor|   Olivia|       20|     Dallas|   TX|\n",
      "|       4|  Taylor|   Olivia|        4|    Houston|   TX|\n",
      "|       6|    null|      Ava|        6|       null|   PA|\n",
      "|       7| Jackson|  William|     null|       null| null|\n",
      "|       9|  Harris|     null|        9|     Dallas|   TX|\n",
      "|       8|   White|   Sophia|       18|   La Jolla|   CA|\n",
      "|       8|   White|   Sophia|        8|  San Diego| null|\n",
      "|      10|    null| Isabella|     null|       null| null|\n",
      "|      11|   Clark|     Emma|       19| Round Rock|   TX|\n",
      "|      11|   Clark|     Emma|       11|     Austin|   TX|\n",
      "|      12|Martinez|      Mia|     null|       null| null|\n",
      "|      13|    null|    Ethan|       13|       null|   FL|\n",
      "|      15|  Walker|     null|     null|       null| null|\n",
      "|      14|     Lee|    James|       14|    Seattle|   WA|\n",
      "+--------+--------+---------+---------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "leftjoin_df=df_person.join(df_address, on=\"personId\", how=\"left\")\n",
    "leftjoin_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Find persons who have no address in the Address table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+---------+---------+-----+\n",
      "|personId|lastName|firstName|addressId|     city|state|\n",
      "+--------+--------+---------+---------+---------+-----+\n",
      "|       3|    null|     Liam|     null|     null| null|\n",
      "|       5|Anderson|     null|     null|     null| null|\n",
      "|       7| Jackson|  William|     null|     null| null|\n",
      "|       8|   White|   Sophia|        8|San Diego| null|\n",
      "|      10|    null| Isabella|     null|     null| null|\n",
      "|      12|Martinez|      Mia|     null|     null| null|\n",
      "|      15|  Walker|     null|     null|     null| null|\n",
      "+--------+--------+---------+---------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "no_addres_df=leftjoin_df.where(col(\"state\").isNull())\n",
    "no_addres_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+---------+----+-----+\n",
      "|personId|lastName|firstName|addressId|city|state|\n",
      "+--------+--------+---------+---------+----+-----+\n",
      "|       3|    null|     Liam|     null|null| null|\n",
      "|       5|Anderson|     null|     null|null| null|\n",
      "|       7| Jackson|  William|     null|null| null|\n",
      "|      10|    null| Isabella|     null|null| null|\n",
      "|      12|Martinez|      Mia|     null|null| null|\n",
      "|      15|  Walker|     null|     null|null| null|\n",
      "+--------+--------+---------+---------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_person.join(df_address, on=\"personId\", how=\"left\").where(col(\"addressId\").isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Count the number of addresses in each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|state|count_state|\n",
      "+-----+-----------+\n",
      "|   CA|          3|\n",
      "|   NY|          2|\n",
      "|   IL|          1|\n",
      "|   TX|          6|\n",
      "|   AZ|          1|\n",
      "|   PA|          1|\n",
      "|   MA|          1|\n",
      "|   WA|          1|\n",
      "|   FL|          1|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "df_address.groupBy(\"state\").agg(count(\"state\").alias(\"count_state\")).where(col(\"state\").isNotNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Find the total number of unique city values in the Address DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_address.select(\"state\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Count how many persons have a NULL value for firstName."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_person.where(col(\"firstName\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+\n",
      "|personId|lastName|firstName|\n",
      "+--------+--------+---------+\n",
      "|       1|   Smith|     John|\n",
      "|       2| Johnson|     null|\n",
      "|       3|    null|     Liam|\n",
      "|       4|  Taylor|   Olivia|\n",
      "|       5|Anderson|     null|\n",
      "|       6|    null|      Ava|\n",
      "|       7| Jackson|  William|\n",
      "|       8|   White|   Sophia|\n",
      "|       9|  Harris|     null|\n",
      "|      10|    null| Isabella|\n",
      "|      11|   Clark|     Emma|\n",
      "|      12|Martinez|      Mia|\n",
      "|      13|    null|    Ethan|\n",
      "|      14|     Lee|    James|\n",
      "|      15|  Walker|     null|\n",
      "+--------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_person.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Add a new column to the Person DataFrame called fullName that combines firstName and lastName."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
