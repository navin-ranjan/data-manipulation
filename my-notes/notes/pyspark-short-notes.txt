--------------- How to get spark config --------------------

spark=SparkSession.builder: Used to initialize or retrieve the current Spark session
spark.sparkContext.getConf().getAll(): Retrieves all key-value pairs of the Spark configuration.

----------------- create schema ------------
from pyspark.sql.types import StructType, StructField, StringType, IntegerType
# Define a schema
schema = StructType([
    StructField("Name", StringType(), True),  # Column 'Name' with StringType and nullable
    StructField("Age", IntegerType(), True),  # Column 'Age' with IntegerType and nullable
    StructField("City", StringType(), True)   # Column 'City' with StringType and nullable
])

----------------- find the null value of column -----------------
# Find rows where lastName is null
null_last_name = df.filter(df.lastName.isNull())

------------------ apply the dataframe where condition --------------------------
# Apply `where` condition to filter rows where `lastName` is "Doe"
filtered_df = df.where(df.lastName == "Doe") #option1
              df.where(col("lastName") == "Doe") #option2 

-------------------- startwith & like function ------

# Filter rows where firstName starts with 'J'
filtered_df = df.where(df.firstName.startswith("J")) #option1
              df.where(df.firstName.like("j%"))    #option2
              df.where(col("firstName").like("J%")) #option3

------------------ dataframe join function --------------

df1.join(df2, on="column_name", how="join_type")
Parameters:
    on: Column name(s) or join condition (e.g., "column_name" or [col1, col2]).
    how: Type of join ("inner", "left", "right", "outer", "semi", "anti", etc.).
Common Join Types:
    inner: Returns rows with matching keys in both DataFrames.
    left: Keeps all rows from the left DataFrame, adds matching rows from the right.
    right: Keeps all rows from the right DataFrame, adds matching rows from the left.
    outer: Returns all rows from both DataFrames, with null for non-matching rows.
    semi: Returns rows from the left DataFrame that have matches in the right.
    anti: Returns rows from the left DataFrame that do not have matches in the right.

--------------- Aggregation Apply in DataFrame --------------------
from pyspark.sql.functions import avg, max, min
select() for Overall Aggregations:
    Use avg(), sum(), count(), etc., directly on the entire DataFrame.
    #df_agg = df.select(avg("Age").alias("Average Age"),sum("Age").alias("Total Age"), count("Name").alias("Total Employees"))

groupBy() for Grouped Aggregations:
    Group data by one or more columns, then apply aggregations using agg().
    #df_address.groupBy("state").agg(count("state").alias("count_state"))
Common Aggregation Functions:
    avg(column): Compute the average.
    sum(column): Compute the total.
    count(column): Count non-null rows.
    max(column), min(column): Compute maximum and minimum values.

Aliasing:
    Use .alias("name") to rename aggregation results.

-----------------
