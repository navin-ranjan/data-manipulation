{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c04429",
   "metadata": {},
   "source": [
    "Q. Solve pyspark question :\n",
    "\n",
    "id (INT), name (STRING), age (INT), salary (FLOAT)\n",
    "\n",
    "Write a PySpark transformation to:\n",
    "\n",
    "1. Filter out rows where age is greater than 30.\n",
    "\n",
    "2. Sort the DataFrame by salary in descending order.\n",
    "\n",
    "3. Select only the name and salary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a819c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+---+------+\n",
      "| id|        name|age|salary|\n",
      "+---+------------+---+------+\n",
      "|  1|navin ranjan| 27| 35000|\n",
      "|  2|rohit ranjan| 23| 28000|\n",
      "|  3|manoj ranjan| 38| 50000|\n",
      "|  4|rakesh kumar| 25| 35000|\n",
      "+---+------------+---+------+\n",
      "\n",
      "+---+------------+---+------+\n",
      "| id|        name|age|salary|\n",
      "+---+------------+---+------+\n",
      "|  3|manoj ranjan| 38| 50000|\n",
      "+---+------------+---+------+\n",
      "\n",
      "+---+------------+---+------+\n",
      "| id|        name|age|salary|\n",
      "+---+------------+---+------+\n",
      "|  3|manoj ranjan| 38| 50000|\n",
      "|  1|navin ranjan| 27| 35000|\n",
      "|  4|rakesh kumar| 25| 35000|\n",
      "|  2|rohit ranjan| 23| 28000|\n",
      "+---+------------+---+------+\n",
      "\n",
      "+------------+------+\n",
      "|        name|salary|\n",
      "+------------+------+\n",
      "|navin ranjan| 35000|\n",
      "|rohit ranjan| 28000|\n",
      "|manoj ranjan| 50000|\n",
      "|rakesh kumar| 35000|\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName('question1').getOrCreate()\n",
    "\n",
    "schema = ['id', 'name', 'age', 'salary']\n",
    "\n",
    "data = [(1, 'navin ranjan', 27, 35000),\n",
    "        (2, 'rohit ranjan', 23, 28000),\n",
    "        (3, 'manoj ranjan', 38, 50000),\n",
    "        (4, 'rakesh kumar', 25, 35000)]\n",
    "\n",
    "df=spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "\n",
    "# Filter out rows where age is greater than 30.\n",
    "fdf=df.filter(col('age')> 30)\n",
    "fdf.show()\n",
    "\n",
    "# Sort the DataFrame by salary in descending order.\n",
    "sdf=df.orderBy(desc(\"age\"))\n",
    "sdf.show()\n",
    "\n",
    "# Select only the name and salary columns.\n",
    "nsdf= df.select(col('name'), col('salary'))\n",
    "nsdf.show()\n",
    "\n",
    "\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c657c04",
   "metadata": {},
   "source": [
    "Q PySpark DataFrame with the following schema:\n",
    " id, name , age , salary\n",
    "1. Register this DataFrame as a temporary view named \"employees\".\n",
    "2. Write a Spark SQL query to select the names of employees who earn more than 30000 and are younger than 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5392f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "|rohit|\n",
      "|rohan|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName('question2').getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"salary\", LongType(), True)\n",
    "])\n",
    "\n",
    "data = [(23, 'navin', 13, 25000),\n",
    "        (20, 'rohit', 17, 46000),\n",
    "        (21, 'rohan', 23, 50000)]\n",
    "\n",
    "df= spark.createDataFrame(data, schema)\n",
    "\n",
    "df.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "sdf=spark.sql(\" select name from employees where salary> 30000 \")\n",
    "\n",
    "sdf.show()\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515effe6",
   "metadata": {},
   "source": [
    "Write PySpark code to:\n",
    "\n",
    "1. Group the data by region and product.\n",
    "2. Calculate the total sales for each group.\n",
    "3. Show the result.\n",
    "\n",
    "columns = ['region', 'product', 'sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04df9bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----+\n",
      "|region|product|sales|\n",
      "+------+-------+-----+\n",
      "|  East|  Apple| 1200|\n",
      "|  East| Banana|  800|\n",
      "|  West|  Apple| 1500|\n",
      "|  West| Banana|  600|\n",
      "|  East|  Apple|  500|\n",
      "+------+-------+-----+\n",
      "\n",
      "+------+-------+---------+\n",
      "|region|product|total_sum|\n",
      "+------+-------+---------+\n",
      "|  East|  Apple|     1700|\n",
      "|  East| Banana|      800|\n",
      "|  West|  Apple|     1500|\n",
      "|  West| Banana|      600|\n",
      "+------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark =SparkSession.builder.appName('question3').getOrCreate()\n",
    "\n",
    "columns = ['region', 'product', 'sales']\n",
    "\n",
    "data = [\n",
    "    ('East', 'Apple', 1200),\n",
    "    ('East', 'Banana', 800),\n",
    "    ('West', 'Apple', 1500),\n",
    "    ('West', 'Banana', 600),\n",
    "    ('East', 'Apple', 500)\n",
    "]\n",
    "\n",
    "df=spark.createDataFrame(data, columns)\n",
    "ndf=df.groupBy(\"region\", \"product\").agg(sum(\"sales\").alias(\"total_sum\"))\n",
    "ndf.show()\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
