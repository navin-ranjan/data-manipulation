{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c04429",
   "metadata": {},
   "source": [
    "Q. Solve pyspark question :\n",
    "\n",
    "id (INT), name (STRING), age (INT), salary (FLOAT)\n",
    "\n",
    "Write a PySpark transformation to:\n",
    "\n",
    "1. Filter out rows where age is greater than 30.\n",
    "\n",
    "2. Sort the DataFrame by salary in descending order.\n",
    "\n",
    "3. Select only the name and salary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a819c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+---+------+\n",
      "| id|        name|age|salary|\n",
      "+---+------------+---+------+\n",
      "|  1|navin ranjan| 27| 35000|\n",
      "|  2|rohit ranjan| 23| 28000|\n",
      "|  3|manoj ranjan| 38| 50000|\n",
      "|  4|rakesh kumar| 25| 35000|\n",
      "+---+------------+---+------+\n",
      "\n",
      "+---+------------+---+------+\n",
      "| id|        name|age|salary|\n",
      "+---+------------+---+------+\n",
      "|  3|manoj ranjan| 38| 50000|\n",
      "+---+------------+---+------+\n",
      "\n",
      "+---+------------+---+------+\n",
      "| id|        name|age|salary|\n",
      "+---+------------+---+------+\n",
      "|  3|manoj ranjan| 38| 50000|\n",
      "|  1|navin ranjan| 27| 35000|\n",
      "|  4|rakesh kumar| 25| 35000|\n",
      "|  2|rohit ranjan| 23| 28000|\n",
      "+---+------------+---+------+\n",
      "\n",
      "+------------+------+\n",
      "|        name|salary|\n",
      "+------------+------+\n",
      "|navin ranjan| 35000|\n",
      "|rohit ranjan| 28000|\n",
      "|manoj ranjan| 50000|\n",
      "|rakesh kumar| 35000|\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName('question1').getOrCreate()\n",
    "\n",
    "schema = ['id', 'name', 'age', 'salary']\n",
    "\n",
    "data = [(1, 'navin ranjan', 27, 35000),\n",
    "        (2, 'rohit ranjan', 23, 28000),\n",
    "        (3, 'manoj ranjan', 38, 50000),\n",
    "        (4, 'rakesh kumar', 25, 35000)]\n",
    "\n",
    "df=spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "\n",
    "# Filter out rows where age is greater than 30.\n",
    "fdf=df.filter(col('age')> 30)\n",
    "fdf.show()\n",
    "\n",
    "# Sort the DataFrame by salary in descending order.\n",
    "sdf=df.orderBy(desc(\"age\"))\n",
    "sdf.show()\n",
    "\n",
    "# Select only the name and salary columns.\n",
    "nsdf= df.select(col('name'), col('salary'))\n",
    "nsdf.show()\n",
    "\n",
    "\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c657c04",
   "metadata": {},
   "source": [
    "Q PySpark DataFrame with the following schema:\n",
    " id, name , age , salary\n",
    "1. Register this DataFrame as a temporary view named \"employees\".\n",
    "2. Write a Spark SQL query to select the names of employees who earn more than 30000 and are younger than 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5392f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "|rohit|\n",
      "|rohan|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName('question2').getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"salary\", LongType(), True)\n",
    "])\n",
    "\n",
    "data = [(23, 'navin', 13, 25000),\n",
    "        (20, 'rohit', 17, 46000),\n",
    "        (21, 'rohan', 23, 50000)]\n",
    "\n",
    "df= spark.createDataFrame(data, schema)\n",
    "\n",
    "df.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "sdf=spark.sql(\" select name from employees where salary> 30000 \")\n",
    "\n",
    "sdf.show()\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515effe6",
   "metadata": {},
   "source": [
    "Write PySpark code to:\n",
    "\n",
    "1. Group the data by region and product.\n",
    "2. Calculate the total sales for each group.\n",
    "3. Show the result.\n",
    "\n",
    "columns = ['region', 'product', 'sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04df9bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----+\n",
      "|region|product|sales|\n",
      "+------+-------+-----+\n",
      "|  East|  Apple| 1200|\n",
      "|  East| Banana|  800|\n",
      "|  West|  Apple| 1500|\n",
      "|  West| Banana|  600|\n",
      "|  East|  Apple|  500|\n",
      "+------+-------+-----+\n",
      "\n",
      "+------+-------+---------+\n",
      "|region|product|total_sum|\n",
      "+------+-------+---------+\n",
      "|  East|  Apple|     1700|\n",
      "|  East| Banana|      800|\n",
      "|  West|  Apple|     1500|\n",
      "|  West| Banana|      600|\n",
      "+------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark =SparkSession.builder.appName('question3').getOrCreate()\n",
    "\n",
    "columns = ['region', 'product', 'sales']\n",
    "\n",
    "data = [\n",
    "    ('East', 'Apple', 1200),\n",
    "    ('East', 'Banana', 800),\n",
    "    ('West', 'Apple', 1500),\n",
    "    ('West', 'Banana', 600),\n",
    "    ('East', 'Apple', 500)\n",
    "]\n",
    "\n",
    "df=spark.createDataFrame(data, columns)\n",
    "ndf=df.groupBy(\"region\", \"product\").agg(sum(\"sales\").alias(\"total_sum\"))\n",
    "ndf.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7ea703",
   "metadata": {},
   "source": [
    "Q Write a Python function to find the element with the highest frequency (i.e., the number that appears the most times) and return both the element and its frequency.\n",
    "\n",
    "nums = [2, 3, 2, 4, 3, 5, 2, 3, 5, 5, 5]\n",
    "\n",
    "O/P - (5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "215b2553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 4\n"
     ]
    }
   ],
   "source": [
    "nums = [2, 3, 2, 4, 3, 5, 2, 3, 5, 5, 5]\n",
    "mp=dict()\n",
    "for x in nums:\n",
    "    mp[x]=mp.get(x, 0) + 1\n",
    "\n",
    "mx=float('-inf')\n",
    "key=float('-inf')\n",
    "for k, v in mp.items():\n",
    "    if v > mx:\n",
    "        mx=v\n",
    "        key=k\n",
    "\n",
    "print(key, mx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366322e7",
   "metadata": {},
   "source": [
    "Q. Write a PySpark program to:\n",
    "\n",
    "1. Join both DataFrames on customer_id.\n",
    "2. Calculate the total amount spent by each customer.\n",
    "3. Show the result as: customer_name, total_amount\n",
    "\n",
    "customer = ['customer_id', 'name']\n",
    "\n",
    "order = ['order_id', 'customer_id', 'amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9e0664f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|customer_id| name|\n",
      "+-----------+-----+\n",
      "|          1|Navin|\n",
      "|          2|Rohit|\n",
      "|          3| Aman|\n",
      "|          4|Sumit|\n",
      "+-----------+-----+\n",
      "\n",
      "+--------+-----------+------+\n",
      "|order_id|customer_id|amount|\n",
      "+--------+-----------+------+\n",
      "|     101|          1|   300|\n",
      "|     102|          2|   450|\n",
      "|     103|          1|   150|\n",
      "|     104|          3|   200|\n",
      "+--------+-----------+------+\n",
      "\n",
      "+-----+------------+\n",
      "| name|total_amount|\n",
      "+-----+------------+\n",
      "|Navin|         450|\n",
      "|Rohit|         450|\n",
      "| Aman|         200|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName('question4').getOrCreate()\n",
    "\n",
    "customer_schema=['customer_id', 'name']\n",
    "customer_data = [\n",
    "    (1, \"Navin\"),\n",
    "    (2, \"Rohit\"),\n",
    "    (3, \"Aman\"),\n",
    "    (4, \"Sumit\")\n",
    "]\n",
    "\n",
    "cdf=spark.createDataFrame(customer_data, customer_schema)\n",
    "cdf.show()\n",
    "\n",
    "order_schema =['order_id', 'customer_id', 'amount']\n",
    "\n",
    "order_data = [\n",
    "    (101, 1, 300),\n",
    "    (102, 2, 450),\n",
    "    (103, 1, 150),\n",
    "    (104, 3, 200)\n",
    "]\n",
    "\n",
    "odf=spark.createDataFrame(order_data, order_schema)\n",
    "odf.show()\n",
    "\n",
    "#join both dataframe\n",
    "jdf=odf.join(cdf, on=\"customer_id\", how=\"inner\")\n",
    "\n",
    "#total count each customer\n",
    "tdf=jdf.groupBy(\"name\").agg(sum(col(\"amount\")).alias(\"total_amount\"))\n",
    "\n",
    "#show the result of total amount by customer\n",
    "tdf.show()\n",
    "\n",
    "\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a493ee8c",
   "metadata": {},
   "source": [
    "Q. Write a Python function to return the number that appears most frequently in the list.\n",
    "If multiple numbers have the same highest frequency, return the largest among them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b5e1340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4\n"
     ]
    }
   ],
   "source": [
    "nums = [4, 5, 6, 5, 4, 3, 5, 4, 4]\n",
    "\n",
    "mp=dict()\n",
    "c=0\n",
    "key=0\n",
    "for x in nums:\n",
    "    mp[x]= mp.get(x, 0) + 1\n",
    "\n",
    "for k, v in mp.items():\n",
    "    if c<v :\n",
    "        c=v\n",
    "        key=k\n",
    "print(key, c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
