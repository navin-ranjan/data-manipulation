{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7507eb1",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### ✅ **1. Introduction to Spark & PySpark**\n",
    "\n",
    "---\n",
    "\n",
    "1. **What is Apache Spark?**  \n",
    "   Apache Spark is an open-source, distributed computing system designed for fast computation. It performs in-memory data processing and is used for big data analytics, ETL processes, machine learning, and real-time data stream processing. Spark supports multiple languages like Python, Java, Scala, and R.\n",
    "\n",
    "---\n",
    "\n",
    "2. **What are the main features of Spark?**  \n",
    "   - In-memory computing for faster execution  \n",
    "   - Support for batch and stream processing  \n",
    "   - Fault tolerance via lineage and DAG  \n",
    "   - APIs in multiple languages (Python, Scala, Java, R)  \n",
    "   - Libraries like Spark SQL, MLlib, GraphX, and Spark Streaming  \n",
    "   - Integration with Hadoop, Hive, HBase, Cassandra, Kafka, etc.\n",
    "\n",
    "---\n",
    "\n",
    "3. **What is PySpark?**   \n",
    "   PySpark is the Python API for Apache Spark. It allows Python developers to leverage Spark’s power for distributed computing using familiar Python syntax. It supports all Spark features like DataFrames, SQL, RDDs, and MLlib.\n",
    "\n",
    "---\n",
    "\n",
    "4. **What are the advantages of Spark over Hadoop MapReduce?**  \n",
    "   - Spark processes data in memory, whereas MapReduce writes to disk after every operation.  \n",
    "   - Spark is significantly faster for iterative computations and machine learning.  \n",
    "   - Spark supports real-time data processing with Structured Streaming.  \n",
    "   - It has higher-level APIs (e.g., DataFrames) that simplify development.\n",
    "\n",
    "---\n",
    "\n",
    "5. **What are the main components of Spark?**   \n",
    "   - **Driver Program**: Manages the Spark application lifecycle  \n",
    "   - **Cluster Manager**: Allocates resources (YARN, Mesos, or Standalone)  \n",
    "   - **Executors**: Run tasks on worker nodes  \n",
    "   - **Tasks**: Smallest unit of work sent to executors  \n",
    "   - **Jobs & Stages**: Logical and physical divisions of Spark workloads\n",
    "\n",
    "---\n",
    "\n",
    "6. **What is the Spark ecosystem?**   \n",
    "   - **Spark Core**: Base engine for scheduling and memory management  \n",
    "   - **Spark SQL**: For querying structured data using SQL or DataFrames  \n",
    "   - **Spark Streaming**: For real-time data processing  \n",
    "   - **MLlib**: Machine learning library  \n",
    "   - **GraphX**: For graph processing\n",
    "\n",
    "---\n",
    "\n",
    "7. **What is the role of the Spark driver and executor?**   \n",
    "   - **Driver**: Manages Spark context, prepares execution plan, and schedules jobs/stages/tasks.  \n",
    "   - **Executors**: Run tasks assigned by the driver and store data for in-memory processing.\n",
    "\n",
    "---\n",
    "\n",
    "8. **What is a Spark application?**   \n",
    "   A Spark application is a user program that uses SparkContext to perform a set of operations. It consists of a driver process and multiple executors working together on a job.\n",
    "\n",
    "---\n",
    "\n",
    "9. **What is lazy evaluation in Spark?**   \n",
    "   Lazy evaluation means Spark doesn’t compute results right away. Transformations are not executed until an action (like `show()`, `collect()`, or `count()`) is called. This allows Spark to optimize execution plans for better performance.\n",
    "\n",
    "---\n",
    "\n",
    "10. **What are Spark jobs, stages, and tasks?**   \n",
    "   - **Job**: Triggered by an action (e.g., `collect`)  \n",
    "   - **Stage**: A set of parallel tasks based on DAG and shuffle boundaries  \n",
    "   - **Task**: Unit of work sent to one executor, performs computation on a partition\n",
    "\n",
    "---\n",
    "\n",
    "11. **How does Spark achieve fault tolerance?**  \n",
    "    Spark achieves fault tolerance primarily through its *lineage information* and *RDD immutability*.\n",
    "\n",
    "    * Each RDD in Spark maintains a record of the operations (transformations) used to build it from source data (called lineage).\n",
    "    * If a partition of an RDD is lost (due to node failure, for example), Spark can recompute only the lost partition by replaying the lineage.\n",
    "    * For DataFrames and Datasets, Spark uses query plans and recomputes intermediate results when required.\n",
    "    * Additionally, Spark can persist/cast RDDs and DataFrames in memory or on disk, and uses checkpointing in streaming scenarios for long lineage chains.\n",
    "---\n",
    "\n",
    "12. **What is the difference between Apache Spark and Apache Flink?**  \n",
    "    **Apache Spark** and **Apache Flink** are both distributed data processing frameworks but differ in architecture and use cases:\n",
    "\n",
    "    | Feature                  | Apache Spark                                     | Apache Flink                                                      |\n",
    "    | ------------------------ | ------------------------------------------------ | ----------------------------------------------------------------- |\n",
    "    | **Data Processing Mode** | Primarily micro-batch (Structured Streaming)     | True real-time streaming                                          |\n",
    "    | **Latency**              | Milliseconds to seconds (higher)                 | Sub-second latency (lower)                                        |\n",
    "    | **Fault Tolerance**      | Lineage and DAG recomputation                    | Distributed snapshots & state management                          |\n",
    "    | **Ease of Use**          | Mature ecosystem, strong community support       | Steeper learning curve                                            |\n",
    "    | **Batch Processing**     | Originally batch-oriented, later added streaming | Originally streaming-oriented, supports batch via bounded streams |\n",
    "    | **Streaming Semantics**  | Exactly-once using checkpointing                 | Exactly-once using native state and snapshots                     |\n",
    "    | **Common Use**           | ETL, batch processing, ML workflows              | Real-time analytics, complex event processing                     |\n",
    "\n",
    "    In summary, Spark is better suited for ETL, batch, and ML use cases, while Flink is ideal for real-time, event-driven, and stateful streaming applications.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7293ac3b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### ✅ **2. Spark Architecture & Concepts**\n",
    "\n",
    "---\n",
    "\n",
    "**1. What is an RDD (Resilient Distributed Dataset)?**  \n",
    "An RDD (Resilient Distributed Dataset) is the fundamental data structure in Apache Spark.\n",
    "It represents an immutable, distributed collection of objects that can be processed in parallel across a cluster.\n",
    "\n",
    "**Key characteristics of RDDs:**\n",
    "\n",
    "* **Resilient**: Can recover from node failures using lineage.\n",
    "* **Distributed**: Data is split across multiple nodes.\n",
    "* **Immutable**: Once created, RDDs cannot be changed.\n",
    "* **Lazy Evaluation**: Operations are not executed until an action is called.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "rdd = spark.sparkContext.parallelize([1, 2, 3, 4])\n",
    "```\n",
    "\n",
    "**Use cases:** When you need fine-grained control over data and operations, especially for low-level transformations.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**2. What is a DataFrame in Spark?**  \n",
    "A DataFrame is a distributed collection of data organized into named columns, similar to a table in a relational database or a DataFrame in Pandas.\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "* Built on top of RDDs, but with schema support.\n",
    "* Optimized by Spark’s Catalyst optimizer for better performance.\n",
    "* Can be queried using SQL or DSL (domain-specific language).\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "df = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**3. What is a Dataset in Spark?**  \n",
    "Datasets are a strongly-typed collection of objects that combine the benefits of RDDs and DataFrames.\n",
    "Available only in **Scala** and **Java**, not in PySpark.\n",
    "\n",
    "* Offers compile-time type safety like RDDs.\n",
    "* Optimized execution like DataFrames.\n",
    "* In PySpark, DataFrame is the primary abstraction.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**4. What is the difference between RDD, DataFrame, and Dataset?**  \n",
    "\n",
    "| Feature               | RDD             | DataFrame          | Dataset (Scala/Java)             |\n",
    "| --------------------- | --------------- | ------------------ | -------------------------------- |\n",
    "| **Abstraction Level** | Low-level       | High-level         | Medium-level (type-safe)         |\n",
    "| **Optimization**      | No optimization | Catalyst Optimizer | Catalyst Optimizer + Type Safety |\n",
    "| **Type Safety**       | Yes             | No                 | Yes                              |\n",
    "| **Ease of Use**       | Verbose         | Simple SQL-like    | Moderate                         |\n",
    "| **Performance**       | Slower          | Faster             | Faster                           |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**5. What is a partition in Spark?**  \n",
    "A partition is a logical division of data across the cluster. Spark breaks data into partitions and processes each partition in parallel.\n",
    "\n",
    "* Each partition is processed by a single task.\n",
    "* Number of partitions impacts parallelism and performance.\n",
    "* You can control partitions using `.repartition()` or `.coalesce()`.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "df.repartition(4)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**6. What is a transformation and an action in Spark?**  \n",
    "\n",
    "* **Transformations** are lazy operations (e.g., `map()`, `filter()`, `groupBy()`) that define a new RDD or DataFrame.\n",
    "* **Actions** trigger the actual computation (e.g., `collect()`, `count()`, `show()`).\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "rdd.map(lambda x: x * 2)  # Transformation\n",
    "rdd.count()               # Action\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**7. What is lineage in Spark?**  \n",
    "Lineage is the sequence of operations that lead to the creation of an RDD or DataFrame.\n",
    "It is used for fault tolerance — if data is lost, Spark can reconstruct it using the lineage graph.\n",
    "\n",
    "**Benefit:** No need to store intermediate data — reduces storage cost and improves fault tolerance.\n",
    "\n",
    "---\n",
    "\n",
    "**8. What is a DAG (Directed Acyclic Graph) in Spark?**  \n",
    "A DAG is a graph where vertices represent RDDs or DataFrames and edges represent transformations.\n",
    "\n",
    "* Spark builds a DAG of stages for execution.\n",
    "* DAG helps in optimization by rearranging or combining transformations.\n",
    "* It prevents recomputation of already executed steps.\n",
    "\n",
    "---\n",
    "\n",
    "**9. What is a shuffle operation?**  \n",
    "A **shuffle** is a data movement process across partitions or nodes triggered by operations like `groupBy()`, `join()`, `distinct()`.\n",
    "\n",
    "* It’s expensive as it involves disk I/O and network communication.\n",
    "* Spark tries to avoid unnecessary shuffles using optimizations.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "df.groupBy(\"id\").sum(\"amount\")  # Triggers shuffle\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**10. What is SparkSession?**  \n",
    "`SparkSession` is the entry point for working with Spark functionality in DataFrame and SQL APIs.\n",
    "Introduced in Spark 2.0 to replace `SQLContext` and `HiveContext`.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**11. What happens when an action is called in Spark?**  \n",
    "When you call an action (like `count()`, `collect()`, or `show()`), Spark evaluates the transformations lazily defined earlier. It:\n",
    "\n",
    "* Constructs a **DAG** (Directed Acyclic Graph) of stages.\n",
    "* Divides the DAG into **stages** based on shuffle boundaries.\n",
    "* Launches **tasks** for each stage across the cluster.\n",
    "* Executes tasks in parallel and returns results.\n",
    "\n",
    "Actions **trigger the entire execution plan**, optimize it, and return the computed result.\n",
    "\n",
    "---\n",
    "\n",
    "**12. How does Spark handle failures during execution?**  \n",
    "Spark uses **lineage information** and **task retry mechanisms** for fault tolerance.\n",
    "\n",
    "* **If a node fails**, Spark reschedules the failed task on another node.\n",
    "* **If data is lost**, Spark recomputes it using the DAG lineage.\n",
    "* Spark retries tasks by default (usually 4 times) before marking them failed.\n",
    "\n",
    "---\n",
    "\n",
    "**13. What is speculative execution in Spark?**  \n",
    "Speculative execution helps avoid slow tasks (stragglers) by running backup copies of the same task on other nodes.\n",
    "\n",
    "* The first task to complete wins; others are killed.\n",
    "* It improves performance in the presence of **noisy or slow nodes**.\n",
    "* Enabled by default for MapReduce-like operations.\n",
    "\n",
    "Enable using:\n",
    "\n",
    "```bash\n",
    "spark.speculation true\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**14. How does Spark handle memory management internally?**  \n",
    "Spark divides memory into two main regions:\n",
    "\n",
    "* **Execution memory**: Used for computation like joins, aggregations, sorting.\n",
    "* **Storage memory**: Used for caching RDDs/DataFrames and broadcast variables.\n",
    "\n",
    "Spark dynamically shares memory between execution and storage. It uses **Unified Memory Management** from Spark 1.6+.\n",
    "\n",
    "Garbage collection and **Tungsten project optimizations** (off-heap memory, bytecode generation) also help with efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "**15. What is an accumulator in Spark and how is it used?**\n",
    "Accumulators are variables that can be used for **aggregating information** across executors in a Spark job.\n",
    "\n",
    "* Mainly used for **debugging**, **metrics**, or **counters**\n",
    "* Updates are only guaranteed to be **seen on the driver**\n",
    "\n",
    "Example in PySpark:\n",
    "\n",
    "```python\n",
    "acc = spark.sparkContext.accumulator(0)\n",
    "\n",
    "def count_nulls(row):\n",
    "    global acc\n",
    "    if row is None:\n",
    "        acc += 1\n",
    "\n",
    "df.foreach(lambda row: count_nulls(row))\n",
    "print(\"Total nulls:\", acc.value)\n",
    "```\n",
    "\n",
    "⚠️ They are **write-only from executors** and **readable only on the driver**.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3fabe1",
   "metadata": {},
   "source": [
    "\n",
    "### ✅ **3. Getting Started with PySpark**\n",
    "\n",
    "---\n",
    "\n",
    "**1. How do you install PySpark?**  \n",
    "You can install PySpark using pip:\n",
    "\n",
    "```bash\n",
    "pip install pyspark\n",
    "```\n",
    "\n",
    "It installs the necessary PySpark binaries and dependencies.\n",
    "For local development, ensure Java 8 or above is installed, and the `JAVA_HOME` environment variable is set.\n",
    "\n",
    "---\n",
    "\n",
    "**2. How do you create a SparkSession in PySpark?**  \n",
    "A `SparkSession` is the entry point to Spark functionality in PySpark:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "This replaces the older `SQLContext` and `HiveContext` APIs.\n",
    "\n",
    "---\n",
    "\n",
    "**3. How do you run PySpark in local mode?**  \n",
    "To run PySpark locally (without a cluster), set the master to `local`:\n",
    "\n",
    "```python\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LocalExample\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "Here, `[*]` uses all available cores on your machine.\n",
    "\n",
    "---\n",
    "\n",
    "**4. How do you submit a PySpark job?**  \n",
    "Use the `spark-submit` command from the terminal:\n",
    "\n",
    "```bash\n",
    "spark-submit my_script.py\n",
    "```\n",
    "\n",
    "You can also specify cluster configurations:\n",
    "\n",
    "```bash\n",
    "spark-submit --master yarn --deploy-mode cluster my_script.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**5. How do you read a CSV file using PySpark?**  \n",
    "You can read a CSV into a DataFrame using:\n",
    "\n",
    "```python\n",
    "df = spark.read.option(\"header\", \"true\").csv(\"path/to/file.csv\")\n",
    "df.show()\n",
    "```\n",
    "\n",
    "Additional options like `inferSchema`, `delimiter`, and `nullValue` can be added.\n",
    "\n",
    "---\n",
    "\n",
    "**6. How do you write a DataFrame to a file?**  \n",
    "You can write a DataFrame in various formats:\n",
    "\n",
    "```python\n",
    "df.write.mode(\"overwrite\").parquet(\"output/path\")\n",
    "df.write.csv(\"output.csv\", header=True)\n",
    "```\n",
    "\n",
    "Use `.mode()` with values like `overwrite`, `append`, `ignore`, `error`.\n",
    "\n",
    "---\n",
    "\n",
    "**7. How do you view the schema of a DataFrame?**  \n",
    "Use the `printSchema()` method:\n",
    "\n",
    "```python\n",
    "df.printSchema()\n",
    "```\n",
    "\n",
    "This displays data types of each column in a tree format.\n",
    "\n",
    "---\n",
    "\n",
    "**8. How do you show the first N rows of a DataFrame?**  \n",
    "Use:\n",
    "\n",
    "```python\n",
    "df.show(n)\n",
    "```\n",
    "\n",
    "By default, `df.show()` displays the first 20 rows. Use `truncate=False` to show full column values:\n",
    "\n",
    "```python\n",
    "df.show(10, truncate=False)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**9. How do you configure PySpark with different memory/executor settings?**  \n",
    "You can configure these during SparkSession creation or via `spark-submit`:\n",
    "\n",
    "```python\n",
    "SparkSession.builder \\\n",
    "    .appName(\"ConfigExample\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "Or from CLI:\n",
    "\n",
    "```bash\n",
    "spark-submit --executor-memory 4G --driver-memory 2G my_script.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**10. How do you integrate PySpark with Jupyter Notebooks?**  \n",
    "Install Jupyter and PySpark:\n",
    "\n",
    "```bash\n",
    "pip install jupyter pyspark\n",
    "```\n",
    "\n",
    "Then launch Jupyter:\n",
    "\n",
    "```bash\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "In the notebook, create a SparkSession:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Notebook\").getOrCreate()\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5478c1c9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### ✅ **4. RDD Operations**\n",
    "\n",
    "---\n",
    "\n",
    "**1. How do you create an RDD in PySpark?**  \n",
    "You can create RDDs in two primary ways:\n",
    "\n",
    "* From an existing collection (like a Python list):\n",
    "\n",
    "```python\n",
    "rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
    "```\n",
    "\n",
    "* From an external data source:\n",
    "\n",
    "```python\n",
    "rdd = spark.sparkContext.textFile(\"path/to/file.txt\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**2. What are common RDD transformations?**  \n",
    "Transformations are **lazy operations** that define a new RDD from an existing one. Common transformations include:\n",
    "\n",
    "* `map()`\n",
    "* `filter()`\n",
    "* `flatMap()`\n",
    "* `distinct()`\n",
    "* `union()`\n",
    "* `intersection()`\n",
    "* `groupByKey()`\n",
    "* `reduceByKey()`\n",
    "* `sortBy()`\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "rdd2 = rdd.map(lambda x: x * 2)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**3. What are common RDD actions?**  \n",
    "Actions trigger the actual execution and return results. Examples include:\n",
    "\n",
    "* `collect()`\n",
    "* `count()`\n",
    "* `first()`\n",
    "* `take(n)`\n",
    "* `reduce()`\n",
    "* `saveAsTextFile()`\n",
    "* `foreach()`\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "rdd.collect()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**4. How do you filter data in an RDD?**  \n",
    "You can use `filter()` with a lambda function:\n",
    "\n",
    "```python\n",
    "filtered_rdd = rdd.filter(lambda x: x % 2 == 0)\n",
    "```\n",
    "\n",
    "This returns an RDD with only even numbers.\n",
    "\n",
    "---\n",
    "\n",
    "**5. How do you map a function over an RDD?**  \n",
    "Use the `map()` transformation:\n",
    "\n",
    "```python\n",
    "squared_rdd = rdd.map(lambda x: x * x)\n",
    "```\n",
    "\n",
    "It applies the function to each element and returns a new RDD.\n",
    "\n",
    "---\n",
    "\n",
    "**6. How do you perform reduce operations on RDDs?**  \n",
    "Use `reduce()` to aggregate elements:\n",
    "\n",
    "```python\n",
    "total = rdd.reduce(lambda a, b: a + b)\n",
    "```\n",
    "\n",
    "This sums all elements of the RDD.\n",
    "\n",
    "---\n",
    "\n",
    "**7. How do you cache or persist an RDD?**  \n",
    "\n",
    "* `cache()` stores the RDD in memory:\n",
    "\n",
    "```python\n",
    "rdd.cache()\n",
    "```\n",
    "\n",
    "* `persist()` gives control over storage levels:\n",
    "\n",
    "```python\n",
    "rdd.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "```\n",
    "\n",
    "Caching is useful when the same RDD is reused multiple times.\n",
    "\n",
    "---\n",
    "\n",
    "**8. How do you repartition or coalesce an RDD?**  \n",
    "\n",
    "* `repartition(n)` increases/decreases partitions with full shuffle:\n",
    "\n",
    "```python\n",
    "rdd2 = rdd.repartition(4)\n",
    "```\n",
    "\n",
    "* `coalesce(n)` reduces partitions with minimal data movement:\n",
    "\n",
    "```python\n",
    "rdd2 = rdd.coalesce(2)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**9. How do you collect data from an RDD?**  \n",
    "Use `collect()` to bring all data to the driver:\n",
    "\n",
    "```python\n",
    "data = rdd.collect()\n",
    "print(data)\n",
    "```\n",
    "\n",
    "⚠️ Not recommended on large datasets—it can cause memory issues.\n",
    "\n",
    "---\n",
    "\n",
    "**10. How do you save an RDD to a file?**  \n",
    "Use `saveAsTextFile()`:\n",
    "\n",
    "```python\n",
    "rdd.saveAsTextFile(\"output/path\")\n",
    "```\n",
    "\n",
    "The result will be written as multiple part files, one per partition.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**11. What are key-value RDDs?**  \n",
    "Key-value RDDs are RDDs where each element is a pair tuple: `(key, value)`. These RDDs are essential for aggregation, grouping, and joining operations.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "kv_rdd = spark.sparkContext.parallelize([(\"a\", 1), (\"b\", 2), (\"a\", 3)])\n",
    "```\n",
    "\n",
    "Key-value RDDs unlock powerful transformations like:\n",
    "\n",
    "* `reduceByKey()`\n",
    "* `groupByKey()`\n",
    "* `combineByKey()`\n",
    "* `aggregateByKey()`\n",
    "* `join()`\n",
    "\n",
    "These are common in log analysis, counts by category, etc.\n",
    "\n",
    "---\n",
    "\n",
    "**12. How do you use aggregateByKey or reduceByKey in RDDs?**  \n",
    "\n",
    "**`reduceByKey()`** merges the values of each key using a reduce function:\n",
    "\n",
    "```python\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 2), (\"a\", 3)])\n",
    "result = rdd.reduceByKey(lambda x, y: x + y).collect()\n",
    "# Output: [('a', 4), ('b', 2)]\n",
    "```\n",
    "\n",
    "**`aggregateByKey()`** gives more flexibility by using:\n",
    "\n",
    "* Initial zero value\n",
    "* Two functions: one for aggregation within a partition and another between partitions\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 2), (\"a\", 3)])\n",
    "result = rdd.aggregateByKey(0, \n",
    "                            lambda acc, v: acc + v,     # seqOp (within partition)\n",
    "                            lambda acc1, acc2: acc1 + acc2)  # combOp (across partitions)\n",
    "```\n",
    "\n",
    "Use `aggregateByKey()` when you need a custom aggregation logic.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b5ec47",
   "metadata": {},
   "source": [
    "\n",
    "### ✅ **5. DataFrame Operations**\n",
    "---\n",
    "\n",
    "**1. How do you create a DataFrame in PySpark?**   \n",
    "You can create a DataFrame from various sources like RDDs, Python collections, or external files.\n",
    "\n",
    "✅ From Python list:\n",
    "```python\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30)]\n",
    "columns = [\"name\", \"age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "```\n",
    "\n",
    "✅ From CSV or other files:\n",
    "```python\n",
    "df = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**2. How do you select columns from a DataFrame?**  \n",
    "You can select single or multiple columns using `select()` or dot notation:\n",
    "```python\n",
    "df.select(\"name\").show()\n",
    "df.select(\"name\", \"age\").show()\n",
    "df.select(df[\"name\"], df[\"age\"]).show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**3. How do you filter rows in a DataFrame?**  \n",
    "Use the `filter()` or `where()` method to filter rows:\n",
    "```python\n",
    "df.filter(df[\"age\"] > 25).show()\n",
    "df.where(\"age > 25\").show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**4. How do you add or drop columns?**  \n",
    "✅ Add a column:\n",
    "```python\n",
    "df = df.withColumn(\"new_col\", df[\"age\"] + 10)\n",
    "```\n",
    "\n",
    "✅ Drop a column:\n",
    "```python\n",
    "df = df.drop(\"new_col\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**5. How do you group and aggregate data?**  \n",
    "Use `groupBy()` along with aggregate functions:\n",
    "```python\n",
    "df.groupBy(\"department\").agg({\"salary\": \"avg\"}).show()\n",
    "\n",
    "# OR using functions\n",
    "from pyspark.sql.functions import avg\n",
    "df.groupBy(\"department\").agg(avg(\"salary\")).show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**6. How do you join DataFrames?**  \n",
    "You can perform various types of joins using `join()`:\n",
    "```python\n",
    "df1.join(df2, df1.id == df2.emp_id, \"inner\").show()\n",
    "# Join types: inner, left, right, outer, left_semi, left_anti\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**7. How do you sort DataFrames?**  \n",
    "Use `orderBy()` or `sort()`:\n",
    "```python\n",
    "df.orderBy(\"age\").show()\n",
    "df.sort(df[\"age\"].desc()).show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**8. How do you handle missing data?**  \n",
    "You can drop or fill missing values:\n",
    "```python\n",
    "df.dropna().show()  # Remove rows with nulls\n",
    "df.fillna({\"age\": 0, \"name\": \"unknown\"}).show()  # Replace nulls\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**9. How do you change column data types?**  \n",
    "Use the `cast()` function:\n",
    "```python\n",
    "from pyspark.sql.functions import col\n",
    "df = df.withColumn(\"age\", col(\"age\").cast(\"double\"))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**10. How do you use UDFs (User Defined Functions) in PySpark?**  \n",
    "Define a custom Python function and register it as a UDF:\n",
    "```python\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def convert_case(s):\n",
    "    return s.upper()\n",
    "\n",
    "convert_udf = udf(convert_case, StringType())\n",
    "df = df.withColumn(\"name_upper\", convert_udf(df[\"name\"]))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**11. How do you cache or persist a DataFrame?**  \n",
    "You can use `.cache()` or `.persist()` to store the DataFrame in memory (or memory+disk) for performance improvement when reused multiple times.\n",
    "\n",
    "```python\n",
    "df.cache()           # Stores in memory\n",
    "df.persist()         # Stores in memory and disk by default\n",
    "df.persist(StorageLevel.DISK_ONLY)  # Custom persistence level\n",
    "```\n",
    "\n",
    "To unpersist:\n",
    "\n",
    "```python\n",
    "df.unpersist()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**12. How do you convert between DataFrame and RDD?**  \n",
    "✅ DataFrame → RDD:\n",
    "\n",
    "```python\n",
    "rdd = df.rdd\n",
    "```\n",
    "\n",
    "✅ RDD → DataFrame:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import Row\n",
    "rdd = spark.sparkContext.parallelize([Row(name=\"Alice\", age=25)])\n",
    "df = spark.createDataFrame(rdd)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**13. What’s the difference between `select`, `selectExpr`, and `withColumn`?**  \n",
    "\n",
    "* `select()` is used to select specific columns:\n",
    "\n",
    "  ```python\n",
    "  df.select(\"name\", \"age\")\n",
    "  ```\n",
    "\n",
    "* `selectExpr()` allows SQL expressions directly:\n",
    "\n",
    "  ```python\n",
    "  df.selectExpr(\"name\", \"age + 5 as new_age\")\n",
    "  ```\n",
    "\n",
    "* `withColumn()` creates a new or modifies existing column:\n",
    "\n",
    "  ```python\n",
    "  df.withColumn(\"age_plus_5\", df[\"age\"] + 5)\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "**14. How do you handle nested or complex columns (arrays, structs, maps)?**  \n",
    "You can access nested fields using dot notation and flatten them:\n",
    "\n",
    "```python\n",
    "df.select(\"user.name\", \"user.address.city\").show()\n",
    "```\n",
    "\n",
    "To explode arrays:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import explode\n",
    "df.select(explode(df[\"hobbies\"])).show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**15. How do you explode arrays in PySpark DataFrames?**  \n",
    "The `explode()` function transforms each element of an array into a separate row.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "data = [(\"A\", [\"Python\", \"SQL\"]), (\"B\", [\"Java\", \"Scala\"])]\n",
    "df = spark.createDataFrame(data, [\"name\", \"skills\"])\n",
    "\n",
    "df_exploded = df.select(\"name\", explode(\"skills\").alias(\"skill\"))\n",
    "df_exploded.show()\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556e2aea",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### ✅ **6. PySpark SQL**\n",
    "\n",
    "---\n",
    "\n",
    "**1. What is Spark SQL?**  \n",
    "Spark SQL is a Spark module that allows users to run SQL queries on structured data using Spark’s distributed computing engine. It supports:\n",
    "\n",
    "* SQL queries using `spark.sql()`\n",
    "* DataFrame API\n",
    "* Integration with Hive\n",
    "* UDFs (User Defined Functions)\n",
    "\n",
    "It provides powerful query optimization via the **Catalyst optimizer** and **Tungsten engine**.\n",
    "\n",
    "---\n",
    "\n",
    "**2. How do you run SQL queries on DataFrames?**  \n",
    "To run SQL queries, you must register the DataFrame as a temporary view:\n",
    "\n",
    "```python\n",
    "df.createOrReplaceTempView(\"employees\")\n",
    "spark.sql(\"SELECT name, salary FROM employees WHERE salary > 50000\").show()\n",
    "```\n",
    "\n",
    "You can also register a **global temporary view**:\n",
    "\n",
    "```python\n",
    "df.createOrReplaceGlobalTempView(\"global_employees\")\n",
    "spark.sql(\"SELECT * FROM global_temp.global_employees\").show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**3. How do you register a DataFrame as a temporary view?**  \n",
    "There are two types:\n",
    "\n",
    "* **Session-Scoped Temporary View**:\n",
    "\n",
    "  ```python\n",
    "  df.createOrReplaceTempView(\"my_view\")\n",
    "  ```\n",
    "\n",
    "* **Global Temporary View** (shared across sessions):\n",
    "\n",
    "  ```python\n",
    "  df.createOrReplaceGlobalTempView(\"global_view\")\n",
    "  ```\n",
    "\n",
    "Then you can query it using `spark.sql()`.\n",
    "\n",
    "---\n",
    "\n",
    "**4. How do you read and write Parquet/ORC/JSON files?**  \n",
    "\n",
    "✅ Reading:\n",
    "\n",
    "```python\n",
    "df_parquet = spark.read.parquet(\"data/emp.parquet\")\n",
    "df_json = spark.read.json(\"data/emp.json\")\n",
    "df_orc = spark.read.orc(\"data/emp.orc\")\n",
    "```\n",
    "\n",
    "✅ Writing:\n",
    "\n",
    "```python\n",
    "df.write.parquet(\"output/emp.parquet\")\n",
    "df.write.json(\"output/emp.json\")\n",
    "df.write.orc(\"output/emp.orc\")\n",
    "```\n",
    "\n",
    "You can also specify format:\n",
    "\n",
    "```python\n",
    "df.write.format(\"parquet\").save(\"output/\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**5. What are catalog functions in Spark SQL?**  \n",
    "Catalog functions provide metadata about Spark objects (databases, tables, functions):\n",
    "\n",
    "```python\n",
    "spark.catalog.listDatabases()\n",
    "spark.catalog.listTables()\n",
    "spark.catalog.listColumns(\"table_name\")\n",
    "spark.catalog.isCached(\"table_name\")\n",
    "```\n",
    "\n",
    "You can also cache/un-cache tables:\n",
    "\n",
    "```python\n",
    "spark.catalog.cacheTable(\"my_table\")\n",
    "spark.catalog.uncacheTable(\"my_table\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**6. How do you use window functions in Spark SQL?**  \n",
    "Window functions perform calculations across rows related to the current row:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col\n",
    "\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "df.withColumn(\"rank\", rank().over(windowSpec)).show()\n",
    "```\n",
    "\n",
    "Common window functions: `rank()`, `dense_rank()`, `row_number()`, `lead()`, `lag()`.\n",
    "\n",
    "---\n",
    "\n",
    "**7. How do you create and use global temp views?**  \n",
    "Global temp views are available across Spark sessions:\n",
    "\n",
    "```python\n",
    "df.createOrReplaceGlobalTempView(\"global_employees\")\n",
    "spark.sql(\"SELECT * FROM global_temp.global_employees\").show()\n",
    "```\n",
    "\n",
    "Note: You must prefix with `global_temp.` to access.\n",
    "\n",
    "---\n",
    "\n",
    "**8. How do you use SQL functions in PySpark?**  \n",
    "PySpark provides many built-in functions via `pyspark.sql.functions`:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import col, upper, length\n",
    "\n",
    "df.select(upper(col(\"name\")), length(col(\"name\"))).show()\n",
    "```\n",
    "\n",
    "You can also use SQL expressions:\n",
    "\n",
    "```python\n",
    "df.selectExpr(\"UPPER(name)\", \"LENGTH(name)\").show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**9. How do you optimize SQL queries in Spark?**  \n",
    "\n",
    "* Use **DataFrames** instead of raw RDDs\n",
    "* Use **broadcast joins** for small tables\n",
    "* Avoid wide transformations\n",
    "* Use **partition pruning**\n",
    "* Enable **predicate pushdown** in file reads\n",
    "* Cache reused data with `.cache()`\n",
    "* Check physical/logical plans:\n",
    "\n",
    "  ```python\n",
    "  df.explain(True)\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "**10. How do you register a UDF in Spark SQL?**  \n",
    "You can create custom UDFs using Python functions and register them with Spark:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def add_ten(x):\n",
    "    return x + 10\n",
    "\n",
    "add_ten_udf = udf(add_ten, IntegerType())\n",
    "df.withColumn(\"new_salary\", add_ten_udf(col(\"salary\"))).show()\n",
    "```\n",
    "\n",
    "Or register it for SQL:\n",
    "\n",
    "```python\n",
    "spark.udf.register(\"addTen\", add_ten, IntegerType())\n",
    "spark.sql(\"SELECT addTen(salary) FROM employees\").show()\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8230aca5",
   "metadata": {},
   "source": [
    "\n",
    "### ✅ **7. Data Sources & File Formats**\n",
    "\n",
    "---\n",
    "\n",
    "**1. What file formats does Spark support?**  \n",
    "Spark supports various file formats for reading and writing:\n",
    "\n",
    "* **Structured formats**: CSV, JSON, Parquet, ORC, Avro\n",
    "* **Semi-structured**: XML (via external libraries)\n",
    "* **Binary**: Images, SequenceFiles\n",
    "* **Custom sources**: JDBC, Hive, Delta Lake\n",
    "\n",
    "---\n",
    "\n",
    "**2. How do you read/write CSV, JSON, Parquet, and Avro files?**  \n",
    "\n",
    "✅ Reading:\n",
    "\n",
    "```python\n",
    "# CSV\n",
    "df_csv = spark.read.option(\"header\", True).csv(\"data/employees.csv\")\n",
    "\n",
    "# JSON\n",
    "df_json = spark.read.json(\"data/employees.json\")\n",
    "\n",
    "# Parquet\n",
    "df_parquet = spark.read.parquet(\"data/employees.parquet\")\n",
    "\n",
    "# Avro (requires spark-avro package)\n",
    "df_avro = spark.read.format(\"avro\").load(\"data/employees.avro\")\n",
    "```\n",
    "\n",
    "✅ Writing:\n",
    "\n",
    "```python\n",
    "df.write.mode(\"overwrite\").json(\"output/employees_json\")\n",
    "df.write.mode(\"append\").parquet(\"output/employees_parquet\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**3. What is schema inference?**\n",
    "Schema inference is the process where Spark automatically detects the column names and data types while reading files (like CSV, JSON):\n",
    "\n",
    "```python\n",
    "df = spark.read.option(\"inferSchema\", True).csv(\"data.csv\")\n",
    "```\n",
    "\n",
    "While convenient, it can be expensive for large files. For production, it's recommended to provide an explicit schema.\n",
    "\n",
    "---\n",
    "\n",
    "**4. How do you specify custom schemas?**  \n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.schema(schema).csv(\"data/employees.csv\")\n",
    "```\n",
    "\n",
    "This improves performance and avoids incorrect type inference.\n",
    "\n",
    "---\n",
    "\n",
    "**5. How do you read data from a database using JDBC?**  \n",
    "\n",
    "```python\n",
    "df = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/hrdb\") \\\n",
    "    .option(\"dbtable\", \"employees\") \\\n",
    "    .option(\"user\", \"root\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .load()\n",
    "```\n",
    "\n",
    "You can also write to a DB using `.write.format(\"jdbc\")`.\n",
    "\n",
    "---\n",
    "\n",
    "**6. How do you handle corrupt or malformed records?**  \n",
    "\n",
    "You can configure how Spark handles bad records using the `mode` option:\n",
    "\n",
    "* `\"PERMISSIVE\"` (default): Corrupt records go into a special column\n",
    "* `\"DROPMALFORMED\"`: Drops bad records\n",
    "* `\"FAILFAST\"`: Fails the job immediately\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "spark.read.option(\"mode\", \"DROPMALFORMED\").json(\"data.json\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**7. How do you partition data when writing files?**  \n",
    "\n",
    "```python\n",
    "df.write.partitionBy(\"department\").parquet(\"output/partitioned/\")\n",
    "```\n",
    "\n",
    "This creates subdirectories for each unique value in the partition column.\n",
    "\n",
    "---\n",
    "\n",
    "**8. How do you read multiple file formats from a directory?**  \n",
    "If the files are of the same format (e.g., CSV), just point Spark to the folder:\n",
    "\n",
    "```python\n",
    "df = spark.read.csv(\"data/multiple_csv/\")\n",
    "```\n",
    "\n",
    "To handle multiple formats, you'd need to read them separately and merge:\n",
    "\n",
    "```python\n",
    "df1 = spark.read.csv(\"data/a.csv\")\n",
    "df2 = spark.read.json(\"data/b.json\")\n",
    "df = df1.unionByName(df2)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**9. What’s the difference between save modes (overwrite, append, etc.)?**  \n",
    "\n",
    "Spark supports the following save modes:\n",
    "\n",
    "* `\"append\"` – Adds to existing data\n",
    "* `\"overwrite\"` – Replaces existing data\n",
    "* `\"error\"` (default) – Fails if path exists\n",
    "* `\"ignore\"` – Skips writing if path exists\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "df.write.mode(\"overwrite\").json(\"output/\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**10. How does Spark handle large files and small files?**  \n",
    "\n",
    "✅ Large files: Spark splits them into **multiple partitions**, allowing parallel processing.\n",
    "\n",
    "✅ Small files: Too many small files can lead to performance bottlenecks (many tasks with low computation).\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "* Use `.repartition(n)` or `.coalesce(n)`\n",
    "* Use `merge` operations in output\n",
    "* Optimize data ingestion with batching\n",
    "\n",
    "---\n",
    "\n",
    "**11. How do you read different file formats using `.read.format()` in Spark?**\n",
    "Spark provides a unified API using `.read.format(\"format\")` to read data from various sources:\n",
    "\n",
    "```python\n",
    "# Read CSV\n",
    "df_csv = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"path/to/file.csv\")\n",
    "\n",
    "# Read Parquet\n",
    "df_parquet = spark.read.format(\"parquet\").load(\"path/to/file.parquet\")\n",
    "\n",
    "# Read JSON\n",
    "df_json = spark.read.format(\"json\").load(\"path/to/file.json\")\n",
    "\n",
    "# Read Avro\n",
    "df_avro = spark.read.format(\"avro\").load(\"path/to/file.avro\")\n",
    "```\n",
    "\n",
    "You can also set options like `delimiter`, `mode`, `inferSchema`, and `compression`.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c40429",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### ✅ **8. Data Cleaning & Transformation**\n",
    "\n",
    "---\n",
    "\n",
    "**1. How do you handle missing or null values?**  \n",
    "\n",
    "You can use the following PySpark functions to handle nulls:\n",
    "\n",
    "* **Drop nulls:**\n",
    "\n",
    "```python\n",
    "df.na.drop()  # Drops rows with any null\n",
    "df.na.drop(how=\"all\")  # Drops rows where all values are null\n",
    "```\n",
    "\n",
    "* **Fill nulls:**\n",
    "\n",
    "```python\n",
    "df.na.fill(0)  # Fill all numeric nulls with 0\n",
    "df.na.fill({\"salary\": 0, \"name\": \"Unknown\"})  # Fill specific columns\n",
    "```\n",
    "\n",
    "* **Replace specific values:**\n",
    "\n",
    "```python\n",
    "df.replace(\"N/A\", None)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**2. How do you drop duplicates in a DataFrame?**  \n",
    "\n",
    "Use `.dropDuplicates()`:\n",
    "\n",
    "```python\n",
    "df.dropDuplicates()\n",
    "df.dropDuplicates([\"id\", \"name\"])  # Drop based on specific columns\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**3. How do you replace values in a DataFrame?**  \n",
    "\n",
    "You can use `.replace()` or `when()`:\n",
    "\n",
    "```python\n",
    "df.replace(\"old_value\", \"new_value\")\n",
    "```\n",
    "\n",
    "Or using `when()` for conditional replacement:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "df = df.withColumn(\"status\", when(col(\"status\") == \"inactive\", \"INACTIVE\").otherwise(col(\"status\")))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**4. How do you split and extract columns?**  \n",
    "\n",
    "For string splitting and extraction:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "df = df.withColumn(\"first_name\", split(df[\"full_name\"], \" \").getItem(0))\n",
    "df = df.withColumn(\"last_name\", split(df[\"full_name\"], \" \").getItem(1))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**5. How do you apply custom functions to columns?**  \n",
    "\n",
    "Use UDFs (User Defined Functions):\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def upper_case(name):\n",
    "    return name.upper()\n",
    "\n",
    "uppercase_udf = udf(upper_case, StringType())\n",
    "df = df.withColumn(\"upper_name\", uppercase_udf(df[\"name\"]))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**6. How do you pivot and unpivot data?**  \n",
    "\n",
    "✅ **Pivot (rows to columns):**\n",
    "\n",
    "```python\n",
    "df.groupBy(\"department\").pivot(\"month\").sum(\"salary\").show()\n",
    "```\n",
    "\n",
    "✅ **Unpivot** (melt): You can use `selectExpr()` for simple unpivoting:\n",
    "\n",
    "```python\n",
    "df_unpivoted = df.selectExpr(\"id\", \"stack(3, 'math', math, 'science', science, 'english', english) as (subject, score)\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**7. How do you sample data in PySpark?**  \n",
    "\n",
    "You can randomly sample rows using `.sample()`:\n",
    "\n",
    "```python\n",
    "df_sample = df.sample(withReplacement=False, fraction=0.2, seed=42)\n",
    "```\n",
    "\n",
    "Or take a fixed number of rows:\n",
    "\n",
    "```python\n",
    "df.limit(10).show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**8. How do you rename columns in PySpark?**  \n",
    "\n",
    "You can rename columns using `.withColumnRenamed()`:\n",
    "\n",
    "```python\n",
    "df = df.withColumnRenamed(\"old_name\", \"new_name\")\n",
    "```\n",
    "\n",
    "To rename multiple columns:\n",
    "\n",
    "```python\n",
    "for col_name in df.columns:\n",
    "    df = df.withColumnRenamed(col_name, col_name.lower())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**9. How do you normalize or scale data in PySpark?**  \n",
    "\n",
    "Use `MinMaxScaler` or `StandardScaler` from `pyspark.ml.feature`:\n",
    "\n",
    "```python\n",
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"salary\"], outputCol=\"features\")\n",
    "df_vector = assembler.transform(df)\n",
    "\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled\")\n",
    "df_scaled = scaler.fit(df_vector).transform(df_vector)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**10. How do you explode nested arrays or structs?**  \n",
    "\n",
    "Use the `explode()` function:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df = df.withColumn(\"exploded_col\", explode(df[\"nested_array\"]))\n",
    "```\n",
    "\n",
    "For nested structs, use dot notation:\n",
    "\n",
    "```python\n",
    "df.select(\"user.name\", \"user.age\").show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**11. How do you flatten nested JSON data?**  \n",
    "\n",
    "First, infer schema from nested JSON, then flatten:\n",
    "\n",
    "```python\n",
    "df = spark.read.json(\"nested.json\")\n",
    "df.select(\"user.name\", \"user.address.city\").show()\n",
    "```\n",
    "\n",
    "You may also use `explode()` and `selectExpr()` to flatten deeply nested data.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746dd9c7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### ✅ **9. Performance Tuning & Optimization**\n",
    "\n",
    "---\n",
    "\n",
    "**1. What is Spark Catalyst Optimizer?**  \n",
    "\n",
    "The **Catalyst Optimizer** is a built-in query optimizer in Spark SQL that transforms logical plans into efficient physical plans. It optimizes SQL queries and DataFrame operations by applying rules like constant folding, predicate pushdown, reordering filters, and more — **improving performance without changing your code.**\n",
    "\n",
    "---\n",
    "\n",
    "**2. What is Tungsten in Spark?**  \n",
    "\n",
    "**Tungsten** is a Spark execution engine enhancement for memory and CPU efficiency. It focuses on:\n",
    "\n",
    "* Binary memory format for faster serialization.\n",
    "* Off-heap memory management (avoiding JVM GC overhead).\n",
    "* Whole-stage code generation to optimize physical execution plans.\n",
    "\n",
    "It allows Spark to process data faster and with lower memory usage.\n",
    "\n",
    "---\n",
    "\n",
    "**3. How do you view and understand Spark execution plans?**  \n",
    "\n",
    "Use:\n",
    "\n",
    "```python\n",
    "df.explain()\n",
    "```\n",
    "\n",
    "Or for a more detailed plan:\n",
    "\n",
    "```python\n",
    "df.explain(True)\n",
    "```\n",
    "\n",
    "This prints the physical and logical plans including optimizations. You can also view it in the Spark UI (`/SQL` tab or `Stages` view).\n",
    "\n",
    "---\n",
    "\n",
    "**4. How do you optimize Spark jobs?**  \n",
    "\n",
    "Some common strategies:\n",
    "\n",
    "* Cache intermediate DataFrames that are reused.\n",
    "* Filter early (predicate pushdown).\n",
    "* Avoid wide transformations like `groupBy` or `join` unless necessary.\n",
    "* Use `broadcast` joins when joining small and large datasets.\n",
    "* Minimize shuffles by controlling partitioning.\n",
    "\n",
    "---\n",
    "\n",
    "**5. What are broadcast joins?**  \n",
    "\n",
    "Broadcast joins send a small table to all nodes so the join happens locally without shuffling the large dataset.\n",
    "\n",
    "Usage:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "df.join(broadcast(small_df), \"key\")\n",
    "```\n",
    "\n",
    "It improves performance dramatically when one of the DataFrames is small.\n",
    "\n",
    "---\n",
    "\n",
    "**6. How do you control partitioning?**  \n",
    "\n",
    "Use:\n",
    "\n",
    "* `.repartition(n)` – increases partitions with shuffle.\n",
    "* `.coalesce(n)` – reduces partitions without shuffle (more efficient).\n",
    "\n",
    "You can also partition while writing data:\n",
    "\n",
    "```python\n",
    "df.write.partitionBy(\"year\", \"month\").parquet(\"path/\")\n",
    "```\n",
    "\n",
    "Controlling partition size helps reduce shuffle and improve parallelism.\n",
    "\n",
    "---\n",
    "\n",
    "**7. How do you tune memory and executor settings?**  \n",
    "\n",
    "Tune these Spark configurations:\n",
    "\n",
    "```bash\n",
    "--executor-memory 4G\n",
    "--executor-cores 2\n",
    "--num-executors 5\n",
    "```\n",
    "\n",
    "These depend on your data size, cluster capacity, and workload. Also adjust:\n",
    "\n",
    "```bash\n",
    "spark.sql.shuffle.partitions\n",
    "spark.memory.fraction\n",
    "```\n",
    "\n",
    "Monitoring with Spark UI helps fine-tune.\n",
    "\n",
    "---\n",
    "\n",
    "**8. How do you avoid data skew?**  \n",
    "\n",
    "Techniques:\n",
    "\n",
    "* **Salting** the skewed key (adding a random suffix).\n",
    "* Use **broadcast joins** where possible.\n",
    "* Use **filter** early to reduce data volume.\n",
    "* Repartition intelligently using `repartition()`.\n",
    "\n",
    "Data skew happens when a few keys have too much data, slowing down specific tasks.\n",
    "\n",
    "---\n",
    "\n",
    "**9. How do you use caching and persistence effectively?**  \n",
    "\n",
    "Cache only when:\n",
    "\n",
    "* You reuse a DataFrame multiple times.\n",
    "* It's expensive to recompute.\n",
    "\n",
    "Usage:\n",
    "\n",
    "```python\n",
    "df.cache()\n",
    "df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "```\n",
    "\n",
    "Remove when no longer needed:\n",
    "\n",
    "```python\n",
    "df.unpersist()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**10. What are best practices for writing efficient Spark code?**  \n",
    "\n",
    "* Avoid collecting data to the driver (`.collect()`).\n",
    "* Use `select()` instead of `*` (read only needed columns).\n",
    "* Filter early to reduce data size.\n",
    "* Minimize transformations that cause shuffling (e.g., `groupBy`, `join`).\n",
    "* Broadcast small tables for joins.\n",
    "* Persist reusable DataFrames.\n",
    "\n",
    "---\n",
    "\n",
    "**11. What is whole-stage code generation?**  \n",
    "\n",
    "Whole-stage code generation is a Spark optimization technique that compiles multiple operations into a single Java method — reducing overhead of interpreted execution. This boosts CPU efficiency and speeds up processing.\n",
    "\n",
    "---\n",
    "\n",
    "**12. What is predicate pushdown?**  \n",
    "\n",
    "Predicate pushdown is the technique where filters (WHERE clause conditions) are pushed down to the data source (e.g., Parquet, JDBC) instead of applying them after reading the data.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "df = spark.read.parquet(\"data/\").filter(\"age > 30\")\n",
    "```\n",
    "\n",
    "Here, the filter is applied while reading — improving performance.\n",
    "\n",
    "---\n",
    "\n",
    "**13. What are wide and narrow transformations?**  \n",
    "\n",
    "* **Narrow transformations:** No data shuffling between partitions (e.g., `map`, `filter`). Faster and more efficient.\n",
    "* **Wide transformations:** Require data shuffle across partitions (e.g., `groupBy`, `join`). More resource-intensive.\n",
    "\n",
    "Understanding these helps you optimize transformations.\n",
    "\n",
    "---\n",
    "\n",
    "**14. How do you debug out-of-memory (OOM) errors in Spark?**  \n",
    "\n",
    "* Increase executor memory (`--executor-memory`).\n",
    "* Avoid wide transformations on massive datasets.\n",
    "* Use `persist(StorageLevel.DISK_ONLY)` if memory is limited.\n",
    "* Use `.repartition()` to balance data.\n",
    "* Use Spark UI to analyze stages/tasks that failed.\n",
    "\n",
    "---\n",
    "\n",
    "**15. What is a broadcast join in Spark and when should it be used?**\n",
    "A broadcast join is an optimization technique where the **smaller DataFrame is broadcast to all executors**, avoiding shuffles.\n",
    "\n",
    "Use when:\n",
    "\n",
    "* One table is **much smaller** (e.g., lookup table)\n",
    "* Spark can **fit the small table in memory**\n",
    "\n",
    "Enable explicitly:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "df_joined = df_large.join(broadcast(df_small), \"key\")\n",
    "```\n",
    "\n",
    "Benefits:\n",
    "\n",
    "* Reduces network shuffle\n",
    "* Faster for small lookups\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d205d29",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### ✅ **10. Spark Streaming & Structured Streaming**\n",
    "\n",
    "---\n",
    "\n",
    "**1. What is Spark Streaming?**  \n",
    "Spark Streaming is a Spark component that enables **real-time data processing** of live data streams. It breaks down incoming data into **micro-batches**, processes them using Spark’s core engine, and outputs the results to sinks like HDFS, databases, or dashboards.\n",
    "\n",
    "---\n",
    "\n",
    "**2. What is Structured Streaming?**  \n",
    "Structured Streaming is a newer, high-level API built on top of Spark SQL. Unlike classic Spark Streaming’s micro-batch abstraction, Structured Streaming allows you to treat streaming data as an **unbounded table**, using familiar SQL/DataFrame operations.\n",
    "\n",
    "Key benefits:\n",
    "\n",
    "* Unified batch and stream processing.\n",
    "* Auto management of state and checkpointing.\n",
    "* Easier and more powerful APIs.\n",
    "\n",
    "---\n",
    "\n",
    "**3. What are the differences between DStreams and Structured Streaming?**  \n",
    "\n",
    "| Feature            | DStreams                    | Structured Streaming            |\n",
    "| ------------------ | --------------------------- | ------------------------------- |\n",
    "| API Level          | RDD-based                   | DataFrame/Dataset-based         |\n",
    "| Abstraction        | Micro-batch RDDs            | Unbounded tables                |\n",
    "| Code Complexity    | Higher                      | Lower                           |\n",
    "| Built-in Optimizer | No Catalyst                 | Uses Catalyst & Tungsten        |\n",
    "| Fault Tolerance    | Manual checkpointing needed | Built-in state management       |\n",
    "| Integration        | Fewer integrations          | Better integration with SQL, ML |\n",
    "\n",
    "---\n",
    "\n",
    "**4. How do you read streaming data in PySpark?**  \n",
    "\n",
    "You can use `.readStream` to read streaming data from sources like Kafka, files, socket, etc.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "df = spark.readStream.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .load(\"path/to/input/folder\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**5. How do you write streaming results?**  \n",
    "\n",
    "Use `.writeStream` to define how the output should be written.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "query = df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "Common sinks: `console`, `memory`, `Kafka`, `Parquet`, etc.\n",
    "\n",
    "---\n",
    "\n",
    "**6. What are output modes in Structured Streaming?**  \n",
    "\n",
    "1. **Append** – Only new rows are written.\n",
    "2. **Complete** – All rows are written every time (e.g., for aggregations).\n",
    "3. **Update** – Only changed rows are written.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    ".writeStream.outputMode(\"complete\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**7. How do you handle late data and watermarking?**  \n",
    "\n",
    "Watermarking allows Structured Streaming to handle **out-of-order or late events**.\n",
    "\n",
    "```python\n",
    "df.withWatermark(\"event_time\", \"10 minutes\")\n",
    "```\n",
    "\n",
    "This ensures the engine waits 10 minutes before assuming no more data will arrive for a given window — avoiding duplicate aggregation.\n",
    "\n",
    "---\n",
    "\n",
    "**8. How do you join streaming and static data?**  \n",
    "\n",
    "You can join a streaming DataFrame with a static DataFrame directly in PySpark.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "stream_df.join(static_df, \"key\")\n",
    "```\n",
    "\n",
    "Only certain join types are allowed (e.g., **inner join**, **left outer join**) depending on streaming direction.\n",
    "\n",
    "---\n",
    "\n",
    "**9. How do you monitor streaming queries?**  \n",
    "\n",
    "* **Spark UI > Streaming Tab** shows the active queries, batch durations, state size, input/output rate, etc.\n",
    "* Programmatically:\n",
    "\n",
    "```python\n",
    "query.status\n",
    "query.lastProgress\n",
    "```\n",
    "\n",
    "You can also enable logging to monitor errors and progress.\n",
    "\n",
    "---\n",
    "\n",
    "**10. What are the checkpointing mechanisms in Spark Streaming?**  \n",
    "\n",
    "Checkpointing helps with **fault tolerance** by storing intermediate states and offsets.\n",
    "\n",
    "* **Metadata checkpointing**: stores info about the running query.\n",
    "* **Data checkpointing**: stores intermediate RDDs or state data.\n",
    "\n",
    "Set it using:\n",
    "\n",
    "```python\n",
    ".writeStream.option(\"checkpointLocation\", \"path/to/dir\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**11. How do you handle exactly-once processing?**  \n",
    "\n",
    "To achieve exactly-once semantics:\n",
    "\n",
    "* Use **idempotent sinks** (like Delta Lake, Kafka).\n",
    "* Enable checkpointing.\n",
    "* Avoid duplicates by maintaining state across batches (stateful operations).\n",
    "  Structured Streaming supports this through **stateful transformations + watermarks**.\n",
    "\n",
    "---\n",
    "\n",
    "**12. What are watermarks and event-time windows?**  \n",
    "\n",
    "* **Watermarks**: Allow the system to discard old data and handle late data gracefully.\n",
    "* **Event-time windows**: Allow aggregations like `groupBy(window(col(\"eventTime\"), \"5 minutes\"))`.\n",
    "\n",
    "Together, they ensure efficient **windowed aggregations** on time-series/streaming data.\n",
    "\n",
    "---\n",
    "\n",
    "**13. How do you maintain state in streaming queries?**  \n",
    "\n",
    "Using **stateful transformations** like:\n",
    "\n",
    "```python\n",
    "grouped_df = df.groupBy(\"user\").agg(...)\n",
    "```\n",
    "\n",
    "Or with **mapGroupsWithState** (for custom state logic):\n",
    "\n",
    "```python\n",
    "df.groupByKey(...).mapGroupsWithState(...)\n",
    "```\n",
    "\n",
    "The state is stored in memory or disk (with checkpointing), and updated with each micro-batch.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f5ea61",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### ✅ **11. Machine Learning with PySpark MLlib**\n",
    "\n",
    "---\n",
    "\n",
    "**1. What is MLlib in Spark?**  \n",
    "MLlib (Machine Learning Library) is Spark’s scalable machine learning library. It provides tools for:\n",
    "\n",
    "* Classification\n",
    "* Regression\n",
    "* Clustering\n",
    "* Collaborative filtering\n",
    "* Dimensionality reduction\n",
    "* Feature engineering\n",
    "* Model selection and evaluation\n",
    "\n",
    "MLlib supports both RDD-based and DataFrame-based APIs, though the DataFrame API is now the primary interface.\n",
    "\n",
    "---\n",
    "\n",
    "**2. What are the main features of MLlib?**  \n",
    "\n",
    "* Scalable and distributed processing\n",
    "* Pipeline support (similar to scikit-learn)\n",
    "* Built-in algorithms for classification, regression, clustering\n",
    "* Feature transformation utilities (e.g., Tokenizer, VectorAssembler)\n",
    "* Model persistence (save/load)\n",
    "* Integration with Spark SQL and DataFrames\n",
    "* Support for tuning and cross-validation\n",
    "\n",
    "---\n",
    "\n",
    "**3. How do you prepare data for ML in PySpark?**  \n",
    "\n",
    "To train models in PySpark, data must be in the form of a DataFrame with:\n",
    "\n",
    "* A **features** column: vector of features (Vector type)\n",
    "* A **label** column: the target/output variable\n",
    "\n",
    "You typically use:\n",
    "\n",
    "* `StringIndexer` for categorical encoding\n",
    "* `VectorAssembler` to combine multiple columns into one feature vector\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=[\"age\", \"salary\"], outputCol=\"features\")\n",
    "final_df = assembler.transform(df)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**4. How do you use feature transformers?**  \n",
    "\n",
    "Feature transformers prepare raw data for modeling. Common ones:\n",
    "\n",
    "* `StringIndexer`: converts string labels into numeric indices\n",
    "* `OneHotEncoder`: converts indices into one-hot encoded vectors\n",
    "* `VectorAssembler`: merges multiple feature columns into a single vector\n",
    "* `StandardScaler`: standardizes features to zero mean and unit variance\n",
    "* `PCA`: for dimensionality reduction\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"gender\", outputCol=\"genderIndex\")\n",
    "df = indexer.fit(df).transform(df)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**5. How do you build and train ML models in PySpark?**  \n",
    "\n",
    "You first create and fit a model using a MLlib Estimator like `LogisticRegression`, `DecisionTreeClassifier`, etc.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "model = lr.fit(training_data)\n",
    "```\n",
    "\n",
    "Then use `.transform()` on test data to make predictions:\n",
    "\n",
    "```python\n",
    "predictions = model.transform(test_data)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**6. How do you evaluate ML models?**  \n",
    "\n",
    "You use `Evaluator` classes such as:\n",
    "\n",
    "* `BinaryClassificationEvaluator`\n",
    "* `MulticlassClassificationEvaluator`\n",
    "* `RegressionEvaluator`\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "```\n",
    "\n",
    "Metrics include:\n",
    "\n",
    "* Area under ROC\n",
    "* Precision, Recall\n",
    "* RMSE, R² (for regression)\n",
    "\n",
    "---\n",
    "\n",
    "**7. How do you save and load ML models?**  \n",
    "\n",
    "All models and pipelines in PySpark are **persistable** using `.save()` and `.load()`.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "model.save(\"path/to/model\")\n",
    "loaded_model = LogisticRegressionModel.load(\"path/to/model\")\n",
    "```\n",
    "\n",
    "For pipelines:\n",
    "\n",
    "```python\n",
    "pipeline.save(\"path/to/pipeline\")\n",
    "```\n",
    "\n",
    "Useful for deployment and retraining.\n",
    "\n",
    "---\n",
    "\n",
    "**8. What are pipelines in PySpark MLlib?**  \n",
    "\n",
    "Pipelines help automate ML workflows. A `Pipeline` consists of a sequence of stages: Transformers and Estimators.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[indexer, assembler, lr])\n",
    "model = pipeline.fit(train_data)\n",
    "```\n",
    "\n",
    "Advantages:\n",
    "\n",
    "* Simplifies preprocessing + training\n",
    "* Ensures consistency across train/test\n",
    "* Supports model tuning and evaluation\n",
    "\n",
    "---\n",
    "\n",
    "**9. How do you tune hyperparameters in PySpark?**  \n",
    "\n",
    "Use `ParamGridBuilder` and `CrossValidator`:\n",
    "\n",
    "```python\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "cvModel = crossval.fit(training_data)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**10. What algorithms are available in MLlib?**  \n",
    "\n",
    "**Classification:**\n",
    "\n",
    "* Logistic Regression\n",
    "* Decision Tree\n",
    "* Random Forest\n",
    "* Gradient-Boosted Trees\n",
    "* Naive Bayes\n",
    "* Multilayer Perceptron\n",
    "\n",
    "**Regression:**\n",
    "\n",
    "* Linear Regression\n",
    "* Decision Tree Regression\n",
    "* Random Forest Regression\n",
    "\n",
    "**Clustering:**\n",
    "\n",
    "* KMeans\n",
    "* Gaussian Mixture\n",
    "* Bisecting K-Means\n",
    "\n",
    "**Recommendation:**\n",
    "\n",
    "* ALS (Alternating Least Squares)\n",
    "\n",
    "**Dimensionality Reduction:**\n",
    "\n",
    "* PCA\n",
    "* ChiSqSelector\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f411271b",
   "metadata": {},
   "source": [
    "\n",
    "### ✅ **12. Databricks Platform**\n",
    "\n",
    "---\n",
    "\n",
    "**1. What is Databricks?**   \n",
    "Databricks is a unified data analytics platform based on Apache Spark. It provides a collaborative environment for:\n",
    "\n",
    "- Big data processing  \n",
    "- Machine learning and AI  \n",
    "- Real-time analytics  \n",
    "- SQL-based analytics  \n",
    "- Streaming analytics  \n",
    "\n",
    "It integrates with cloud storage and other Azure/AWS services and offers auto-scaling, notebooks, workflows, and Delta Lake.\n",
    "\n",
    "---\n",
    "\n",
    "**2. What are the main features of Databricks?**\n",
    " \n",
    "- **Interactive Notebooks** for collaborative development\n",
    "- **Auto-scaling Clusters** with Apache Spark\n",
    "- **Support for multiple languages**: Python, SQL, Scala, R\n",
    "- **Built-in MLflow** for experiment tracking\n",
    "- **Delta Lake support** for ACID transactions\n",
    "- **Unity Catalog** for data governance\n",
    "- **Databricks SQL & Dashboards** for BI reporting\n",
    "- **Integration with Git & CI/CD tools**\n",
    "- **Optimized performance using Photon engine**\n",
    "\n",
    "---\n",
    "\n",
    "**3. How do you create and manage Databricks notebooks?**  \n",
    "\n",
    "- You can create notebooks in the Databricks workspace UI.\n",
    "- Choose language: Python, Scala, SQL, or R.\n",
    "- Notebooks can include code, visualizations, and markdown.\n",
    "- You can schedule jobs from notebooks and export them to `.dbc` or `.ipynb`.\n",
    "\n",
    "Shortcuts:\n",
    "- `Shift + Enter`: Run a cell\n",
    "- `%python`, `%sql`, `%scala`: Magic commands for multi-language support\n",
    "\n",
    "---\n",
    "\n",
    "**4. How do you run Spark jobs on Databricks?**  \n",
    "\n",
    "- Attach your notebook to a running **Databricks cluster**.\n",
    "- Use standard Spark APIs in PySpark or Scala.\n",
    "- Jobs can be scheduled or triggered manually.\n",
    "- For production jobs, use **Databricks Jobs** with task orchestration.\n",
    "\n",
    "You can also submit jobs via REST API or CLI.\n",
    "\n",
    "---\n",
    "\n",
    "**5. What is a Databricks cluster?**  \n",
    "\n",
    "A Databricks cluster is a set of compute resources for running Spark jobs.\n",
    "\n",
    "Types:\n",
    "- **Interactive Clusters**: Used for development (attached to notebooks).\n",
    "- **Job Clusters**: Automatically created/destroyed when running scheduled jobs.\n",
    "\n",
    "Cluster configurations:\n",
    "- Node type (VM size)\n",
    "- Auto-scaling\n",
    "- Number of workers\n",
    "- Libraries (PyPI, Maven, etc.)\n",
    "- Init scripts\n",
    "- Logging options\n",
    "\n",
    "---\n",
    "\n",
    "**6. How do you install libraries in Databricks?**  \n",
    "\n",
    "- Via UI: Cluster > Libraries > Install New\n",
    "- From PyPI, Maven, or upload your own `.whl` or `.jar` files\n",
    "- Programmatically with `%pip install` or `%conda` in notebooks\n",
    "\n",
    "You can also use **Init scripts** for installation during cluster startup.\n",
    "\n",
    "---\n",
    "\n",
    "**7. How do you use Databricks Jobs and Workflows?**  \n",
    "\n",
    "Databricks Jobs allow you to run notebooks, JARs, or Python scripts as **scheduled or triggered workflows**.\n",
    "\n",
    "Features:\n",
    "- Multi-task workflows (DAG-based)\n",
    "- Retry policies\n",
    "- Task dependencies\n",
    "- Email notifications\n",
    "- Cluster re-use or per-task cluster\n",
    "\n",
    "You can create jobs via UI, API, or Terraform.\n",
    "\n",
    "---\n",
    "\n",
    "**8. How do you manage data with Databricks DBFS?**  \n",
    "\n",
    "DBFS (Databricks File System) is an abstraction over cloud storage (e.g., ADLS, S3).\n",
    "\n",
    "- Accessible via `/dbfs/`\n",
    "- Supports upload/download of files\n",
    "- Accessible via `%fs` magic command or `dbutils.fs`\n",
    "\n",
    "Example:\n",
    "```python\n",
    "dbutils.fs.ls(\"/FileStore/tables\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**9. How do you use Databricks Repos and version control?**  \n",
    "\n",
    "- Repos allow you to integrate Git providers (GitHub, GitLab, Azure DevOps).\n",
    "- Enables version control and collaboration inside Databricks.\n",
    "- Use UI or Git CLI to sync code.\n",
    "- Supports notebooks and .py/.scala files.\n",
    "\n",
    "---\n",
    "\n",
    "**10. How do you schedule jobs in Databricks?**  \n",
    "\n",
    "Jobs can be scheduled:\n",
    "- **From notebooks or workflows**\n",
    "- Using **time-based schedules** (e.g., daily at 6AM)\n",
    "- Based on **event triggers** (e.g., file arrival)\n",
    "- Programmatically via API\n",
    "\n",
    "You can configure retry settings, email alerts, and access control for job runs.\n",
    "\n",
    "---\n",
    "\n",
    "**11. How do you use Databricks REST API?**  \n",
    "\n",
    "The REST API allows full control over:\n",
    "\n",
    "- Jobs (create, list, run)\n",
    "- Clusters (start, terminate)\n",
    "- DBFS file operations\n",
    "- Secrets and workspace items\n",
    "- Token generation\n",
    "\n",
    "Example using `curl`:\n",
    "```bash\n",
    "curl -X GET https://<databricks-instance>/api/2.0/clusters/list \\\n",
    "  -H \"Authorization: Bearer <token>\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**12. How do you monitor and debug jobs in Databricks?**  \n",
    "\n",
    "Use:\n",
    "- **Job Run History** and logs (stdout/stderr)\n",
    "- **Spark UI** (Stages, Tasks, DAG)\n",
    "- **Ganglia metrics** for system-level performance\n",
    "- **Driver and executor logs** for detailed debugging\n",
    "- **Event Logs** and **Audit Logs** in Unity Catalog\n",
    "\n",
    "---\n",
    "\n",
    "**13. How do you use Databricks SQL and dashboards?**  \n",
    "\n",
    "Databricks SQL:\n",
    "- Allows SQL analysts to query data\n",
    "- Supports BI integrations (Power BI, Tableau)\n",
    "- Save queries and build dashboards\n",
    "\n",
    "Dashboards:\n",
    "- Visual components (bar chart, line, pie, tables)\n",
    "- Schedule refresh\n",
    "- Share with teams\n",
    "\n",
    "---\n",
    "\n",
    "**14. How do you connect Databricks to external data sources?**  \n",
    "\n",
    "- **JDBC/ODBC** for databases\n",
    "- **S3/ADLS** for cloud storage\n",
    "- **Azure Synapse, SQL Server, MySQL, Snowflake, Redshift**\n",
    "\n",
    "Connections can be managed using:\n",
    "- Mount points\n",
    "- `spark.read.format(\"jdbc\")`\n",
    "- Secret scopes for credentials\n",
    "\n",
    "---\n",
    "\n",
    "**15. What are Databricks Delta Lake and its benefits?**   \n",
    "\n",
    "Delta Lake adds ACID transactions, schema enforcement, and time travel to Spark.\n",
    "\n",
    "Benefits:\n",
    "- **Atomicity** with merge/upsert operations\n",
    "- **Schema enforcement and evolution**\n",
    "- **Time travel** for historical data access\n",
    "- **Efficient updates and deletes**\n",
    "- **Better performance with Z-ordering and caching**\n",
    "\n",
    "---\n",
    "\n",
    "**16. What are the different cluster modes in Databricks   (interactive, job)?**  \n",
    "\n",
    "- **Interactive Clusters**: For development & experimentation\n",
    "- **Job Clusters**: Created by jobs and terminated after use\n",
    "\n",
    "Choose job clusters for cost-efficiency and production tasks.\n",
    "\n",
    "---\n",
    "\n",
    "**17. What is Unity Catalog and how is it used?**  \n",
    "\n",
    "Unity Catalog provides centralized metadata management and data governance.\n",
    "\n",
    "Features:\n",
    "- Fine-grained access control (RBAC)\n",
    "- Centralized audit logs\n",
    "- Cross-workspace catalog\n",
    "- Column-level permissions\n",
    "- Data lineage tracking\n",
    "\n",
    "---\n",
    "\n",
    "**18. How does Databricks handle user access and RBAC?**  \n",
    "\n",
    "- **Workspace-level**: Admin, user, read-only roles\n",
    "- **Cluster-level**: Who can start/attach/terminate clusters\n",
    "- **Table-level** (Unity Catalog): SQL grants (`GRANT SELECT ON ...`)\n",
    "- **Secret scopes**: Control access to secrets\n",
    "\n",
    "---\n",
    "\n",
    "**19. How do you manage compute costs in Databricks?**  \n",
    "\n",
    "- Use **auto-termination** and **auto-scaling**\n",
    "- Prefer **job clusters** over all-purpose clusters\n",
    "- Monitor usage via **Cost Management Dashboards**\n",
    "- Use **Photon Runtime** for performance boosts\n",
    "- Turn off unused features (e.g., Unity Catalog on dev)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2a306e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### ✅ **13. Delta Lake**\n",
    "\n",
    "---\n",
    "\n",
    "**1. What is Delta Lake?**   \n",
    "Delta Lake is an open-source storage layer that brings **ACID transactions**, **schema enforcement**, and **time travel** to Apache Spark and big data workloads. It is built on top of Parquet files and integrates tightly with Spark to provide reliable and scalable data lakes.\n",
    "\n",
    "---\n",
    "\n",
    "**2. What are the benefits of Delta Lake over Parquet?**  \n",
    "\n",
    "| Feature                    | Parquet                | Delta Lake             |\n",
    "|---------------------------|------------------------|------------------------|\n",
    "| ACID Transactions         | ❌ No                  | ✅ Yes                 |\n",
    "| Time Travel               | ❌ No                  | ✅ Yes (`VERSION AS OF`)|\n",
    "| Schema Evolution          | ❌ Manual               | ✅ Auto-supported      |\n",
    "| Upsert (MERGE INTO)       | ❌ Complex              | ✅ Simple & efficient  |\n",
    "| Data Lineage              | ❌ No                  | ✅ Yes (Delta Logs)     |\n",
    "| CDC (Change Data Capture) | ❌ Not native          | ✅ Built-in            |\n",
    "\n",
    "---\n",
    "\n",
    "**3. How do you create Delta tables?**  \n",
    "\n",
    "You can create a Delta table in Databricks or PySpark using:\n",
    "\n",
    "```python\n",
    "df.write.format(\"delta\").save(\"/mnt/datalake/sales_delta\")\n",
    "```\n",
    "\n",
    "Or register it as a managed table:\n",
    "\n",
    "```sql\n",
    "CREATE TABLE sales\n",
    "USING DELTA\n",
    "LOCATION '/mnt/datalake/sales_delta';\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**4. How do you perform ACID transactions in Delta Lake?**  \n",
    "\n",
    "Delta Lake maintains a **transaction log** (`_delta_log`) that records all changes. You can:\n",
    "\n",
    "- **INSERT**, **UPDATE**, **DELETE**, or **MERGE** data\n",
    "- Multiple users can safely read/write concurrently\n",
    "- Rollbacks and retries are consistent and recoverable\n",
    "\n",
    "Example of MERGE:\n",
    "```sql\n",
    "MERGE INTO target_table t\n",
    "USING updates u\n",
    "ON t.id = u.id\n",
    "WHEN MATCHED THEN UPDATE SET t.name = u.name\n",
    "WHEN NOT MATCHED THEN INSERT (id, name) VALUES (u.id, u.name);\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**5. How do you update and delete data in Delta tables?**  \n",
    "\n",
    "Update Example:\n",
    "```python\n",
    "spark.sql(\"\"\"\n",
    "  UPDATE sales_delta\n",
    "  SET revenue = revenue * 1.1\n",
    "  WHERE region = 'East'\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "Delete Example:\n",
    "```python\n",
    "spark.sql(\"\"\"\n",
    "  DELETE FROM sales_delta\n",
    "  WHERE date < '2022-01-01'\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**6. What is time travel in Delta Lake?**  \n",
    "\n",
    "Delta Lake supports **Time Travel**, which lets you access previous versions of data using:\n",
    "\n",
    "- **VERSION AS OF** (by version number)\n",
    "- **TIMESTAMP AS OF** (by datetime)\n",
    "\n",
    "Example:\n",
    "```sql\n",
    "SELECT * FROM sales VERSION AS OF 5;\n",
    "```\n",
    "\n",
    "You can also use:\n",
    "```python\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\", 3).load(\"path_to_table\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**7. How do you optimize Delta tables?**  \n",
    "\n",
    "Use the `OPTIMIZE` command to compact small files into larger ones, improving query performance.\n",
    "\n",
    "```sql\n",
    "OPTIMIZE sales\n",
    "```\n",
    "\n",
    "For sorting files to enhance filter performance:\n",
    "\n",
    "```sql\n",
    "OPTIMIZE sales ZORDER BY (customer_id)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**8. How do you handle schema evolution in Delta Lake?**  \n",
    "\n",
    "Delta Lake supports automatic schema evolution when:\n",
    "\n",
    "- Inserting new columns\n",
    "- Changing column types (in some cases)\n",
    "\n",
    "Enable with:\n",
    "```python\n",
    "df.write.option(\"mergeSchema\", \"true\").format(\"delta\").mode(\"overwrite\").save(\"/path\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**9. How do you use Delta Lake with streaming data?**  \n",
    "\n",
    "You can read and write Delta tables as streaming sources/sinks:\n",
    "\n",
    "**Streaming read:**\n",
    "```python\n",
    "spark.readStream.format(\"delta\").load(\"/path_to_delta_table\")\n",
    "```\n",
    "\n",
    "**Streaming write:**\n",
    "```python\n",
    "stream_df.writeStream\n",
    "  .format(\"delta\")\n",
    "  .outputMode(\"append\")\n",
    "  .option(\"checkpointLocation\", \"/path_to_checkpoint\")\n",
    "  .start(\"/path_to_delta_table\")\n",
    "```\n",
    "\n",
    "Delta provides **exactly-once** guarantees for stream processing.\n",
    "\n",
    "---\n",
    "\n",
    "**10. How does Delta Lake support upserts (merge)?**  \n",
    "\n",
    "Delta Lake enables `MERGE INTO` operation for upsert logic.\n",
    "\n",
    "Example:\n",
    "```sql\n",
    "MERGE INTO customers AS target\n",
    "USING updates AS source\n",
    "ON target.id = source.id\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET target.name = source.name\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (id, name) VALUES (source.id, source.name);\n",
    "```\n",
    "\n",
    "This simplifies CDC and real-time data warehousing use cases.\n",
    "\n",
    "---\n",
    "\n",
    "**11. What is Z-order clustering and why is it used?**  \n",
    "\n",
    "`ZORDER` is a file-level optimization in Delta Lake that co-locates related data within files to speed up query filtering.\n",
    "\n",
    "Example:\n",
    "```sql\n",
    "OPTIMIZE orders ZORDER BY (customer_id, order_date)\n",
    "```\n",
    "\n",
    "Used when you filter on multiple columns often – it helps minimize the number of files read during a query.\n",
    "\n",
    "---\n",
    "\n",
    "**12. What are `OPTIMIZE`, `VACUUM`, and `REORG TABLE` used for?**  \n",
    "\n",
    "- **`OPTIMIZE`**: Compacts small files and organizes data (especially with `ZORDER`)\n",
    "- **`VACUUM`**: Physically removes old files (default retention is 7 days)\n",
    "  ```sql\n",
    "  VACUUM sales RETAIN 168 HOURS;\n",
    "  ```\n",
    "- **`REORG TABLE`**: Alias to optimize in some enterprise platforms (not default in Databricks)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e0b4af",
   "metadata": {},
   "source": [
    "\n",
    "### ✅ **14. Security & Best Practices**\n",
    "\n",
    "---\n",
    "\n",
    "**1. How do you secure data in Spark and Databricks?**  \n",
    "Security in Spark and Databricks can be implemented at several levels:\n",
    "\n",
    "* **Authentication**: SSO (Single Sign-On) using Azure AD or other IdPs.\n",
    "* **Authorization**: Role-Based Access Control (RBAC) to manage user privileges.\n",
    "* **Data Encryption**: Data is encrypted **at rest** using storage encryption (e.g., Azure-managed keys) and **in transit** using TLS.\n",
    "* **Token-based Access**: Personal Access Tokens (PAT) or Azure Active Directory tokens for APIs.\n",
    "\n",
    "---\n",
    "\n",
    "**2. What are access controls in Databricks?**  \n",
    "Databricks supports **Unity Catalog**, which centralizes access control across:\n",
    "\n",
    "* **Workspaces**\n",
    "* **Clusters**\n",
    "* **Data objects** (Tables, Views, Schemas, Catalogs)\n",
    "\n",
    "You can assign **user roles** and manage access via:\n",
    "\n",
    "* SQL GRANT statements\n",
    "* Admin Console\n",
    "* Azure/Cloud RBAC integration\n",
    "\n",
    "Example:\n",
    "\n",
    "```sql\n",
    "GRANT SELECT ON TABLE sales TO `data_analyst`;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**3. How do you manage secrets in Databricks?**  \n",
    "Secrets like credentials, API keys, or DB passwords can be stored in **Databricks Secret Scopes**:\n",
    "\n",
    "* Use the **Databricks CLI** or UI to create secret scopes.\n",
    "* Secrets are accessed in notebooks using:\n",
    "\n",
    "  ```python\n",
    "  dbutils.secrets.get(scope=\"my-scope\", key=\"db-password\")\n",
    "  ```\n",
    "* Optionally integrate with **Azure Key Vault** for external management.\n",
    "\n",
    "---\n",
    "\n",
    "**4. What are best practices for Spark job development?**  \n",
    "\n",
    "* **Avoid shuffling large datasets unnecessarily**.\n",
    "* **Cache** intermediate DataFrames only when reused multiple times.\n",
    "* Use **DataFrames** over RDDs for optimization.\n",
    "* Apply **filtering as early as possible** (pushdown predicate).\n",
    "* Use **partitioning** and **bucketing** for large datasets.\n",
    "* Use **broadcast joins** for small lookup tables.\n",
    "* Always **test locally** on a subset before scaling to production.\n",
    "\n",
    "---\n",
    "\n",
    "**5. How do you monitor and log Spark applications?**  \n",
    "\n",
    "* Use **Spark UI** to inspect jobs, stages, and tasks.\n",
    "* In Databricks:\n",
    "\n",
    "  * View **Job Runs** and their detailed metrics.\n",
    "  * Use **driver and executor logs** for debugging.\n",
    "* Enable **Ganglia/Spark metrics** to monitor cluster health.\n",
    "* Use **log4j.properties** for custom logging configuration.\n",
    "\n",
    "---\n",
    "\n",
    "**6. How do you handle sensitive data in Spark?**  \n",
    "\n",
    "* Mask or encrypt sensitive columns (e.g., PII).\n",
    "* Use **column-level access controls** via Unity Catalog.\n",
    "* Store secrets in **Databricks Secrets** instead of hardcoding them.\n",
    "* Apply **data anonymization** techniques before storing in open datasets.\n",
    "* Implement **audit logging** to track data access and operations.\n",
    "\n",
    "---\n",
    "\n",
    "**7. What is Unity Catalog and how does it help with governance?**  \n",
    "\n",
    "Unity Catalog is a unified governance solution in Databricks that:\n",
    "\n",
    "* Centralizes **data access control** and **audit logging**.\n",
    "* Enforces **fine-grained permissions** across workspaces.\n",
    "* Provides a single **metastore** for all your data.\n",
    "* Enables **lineage tracking** for datasets and transformations.\n",
    "* Supports **multi-cloud environments** with consistent policies.\n",
    "\n",
    "---\n",
    "\n",
    "**8. What is the principle of least privilege in Spark/Databricks?**  \n",
    "\n",
    "This principle ensures that users are only granted **the minimum permissions necessary** to perform their tasks.\n",
    "\n",
    "For example:\n",
    "\n",
    "* Developers can read staging data but not production data.\n",
    "* Analysts can query tables but not modify schemas.\n",
    "* Jobs run with service principals having access only to needed resources.\n",
    "\n",
    "This reduces the risk of **accidental data exposure** or **unauthorized access**.\n",
    "\n",
    "---\n",
    "\n",
    "**9. How do you handle logging in Databricks notebooks?**  \n",
    "\n",
    "* Use Python’s built-in `logging` module:\n",
    "\n",
    "  ```python\n",
    "  import logging\n",
    "  logging.basicConfig(level=logging.INFO)\n",
    "  logging.info(\"Processing started...\")\n",
    "  ```\n",
    "* Use `%sh` cell magic for system logs.\n",
    "* Leverage **structured logs** via JSON log format for downstream parsing.\n",
    "* Use **dbutils.fs.put** to store logs in ADLS or DBFS.\n",
    "\n",
    "---\n",
    "\n",
    "**10. How do you manage audit logs in Databricks?**  \n",
    "\n",
    "* Enable **Audit Logs** via **Azure Monitor** or **AWS CloudTrail** depending on platform.\n",
    "* Audit logs include:\n",
    "\n",
    "  * Login attempts\n",
    "  * Job executions\n",
    "  * Notebook access\n",
    "  * Table access\n",
    "* Export logs to **Log Analytics**, **SIEM tools**, or **Blob Storage** for analysis.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907c575d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### ✅ **Section 15: Advanced Topics**\n",
    "\n",
    "---\n",
    "\n",
    "**1. What is GraphFrames in Spark?**   \n",
    "GraphFrames is a Spark API built on top of DataFrames that allows for **graph processing** and **graph-parallel computation**.\n",
    "\n",
    "- It models data as **vertices** (nodes) and **edges** (relationships).\n",
    "- Provides graph algorithms like **PageRank**, **Connected Components**, **Breadth-First Search (BFS)**.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "from graphframes import GraphFrame\n",
    "vertices = spark.createDataFrame([(1, \"A\"), (2, \"B\")], [\"id\", \"name\"])\n",
    "edges = spark.createDataFrame([(1, 2, \"knows\")], [\"src\", \"dst\", \"relationship\"])\n",
    "g = GraphFrame(vertices, edges)\n",
    "g.inDegrees.show()\n",
    "```\n",
    "\n",
    "Use case: Social networks, recommendation engines, network topology.\n",
    "\n",
    "---\n",
    "\n",
    "**2. How do you use SparkR?**   \n",
    "**SparkR** is an R package that provides a frontend to use Apache Spark with R language.\n",
    "\n",
    "- Supports distributed **DataFrame** operations and **MLlib**.\n",
    "- Can use `sparkR.session()` to start a session.\n",
    "- Commonly used for **data analysis**, **ETL**, and **ML** in R environments.\n",
    "\n",
    "Example:\n",
    "```r\n",
    "df <- read.df(\"data.csv\", \"csv\", header=\"true\", inferSchema=\"true\")\n",
    "head(select(df, df$column1))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**3. How do you integrate Spark with BI tools?**  \n",
    "\n",
    "Spark can be integrated with tools like:\n",
    "\n",
    "- **Power BI / Tableau**: via **ODBC/JDBC** connectors.\n",
    "- **Databricks SQL endpoint**: expose SQL-compatible interface.\n",
    "- **Apache Superset**: for open-source dashboards.\n",
    "- Publish preprocessed results to **SQL warehouses** or **Delta Tables** for reporting.\n",
    "\n",
    "---\n",
    "\n",
    "**4. How do you use Spark with cloud storage (S3, ADLS, GCS)?**  \n",
    "\n",
    "Spark integrates natively with cloud storage:\n",
    "\n",
    "- **S3** (AWS): `s3a://bucket-name/path/`\n",
    "- **ADLS Gen2** (Azure): via `abfss://` with OAuth credentials or managed identity.\n",
    "- **GCS** (Google): via `gs://` with service account keys.\n",
    "\n",
    "You must configure Spark with:\n",
    "- Hadoop-compatible drivers\n",
    "- Cloud-specific credentials\n",
    "- Spark configs like `fs.s3a.access.key`, `fs.azure.account.key`, etc.\n",
    "\n",
    "---\n",
    "\n",
    "**5. How do you use Spark with Kubernetes?**  \n",
    "\n",
    "Spark-on-Kubernetes allows running Spark jobs in containers:\n",
    "\n",
    "- Use the `spark-submit` CLI with Kubernetes master:\n",
    "  ```bash\n",
    "  spark-submit \\\n",
    "    --master k8s://https://<k8s-master> \\\n",
    "    --deploy-mode cluster \\\n",
    "    --conf spark.kubernetes.container.image=<image> \\\n",
    "    ...\n",
    "  ```\n",
    "- Spark driver and executors run as pods.\n",
    "- Benefits: scalability, resource isolation, and cloud-native deployment.\n",
    "\n",
    "---\n",
    "\n",
    "**6. What is Koalas and how does it relate to Pandas?**  \n",
    "\n",
    "**Koalas** was a library that provided a **Pandas-like API** on top of PySpark DataFrames.\n",
    "\n",
    "- Bridged the gap between **Pandas** and **Spark**.\n",
    "- Deprecated in favor of **Pandas API on Spark**, now built-in in Spark 3.2+.\n",
    "  ```python\n",
    "  import pyspark.pandas as ps\n",
    "  psdf = ps.read_csv(\"data.csv\")\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "**7. How do you use Databricks Connect?**  \n",
    "\n",
    "**Databricks Connect** allows you to:\n",
    "\n",
    "- Develop Spark code in local IDE (VSCode, PyCharm).\n",
    "- Run the code remotely on **Databricks clusters**.\n",
    "- Benefits:\n",
    "  - Faster local dev cycle\n",
    "  - Full power of cloud compute\n",
    "\n",
    "You configure `databricks-connect` with cluster settings and use `SparkSession` as normal.\n",
    "\n",
    "---\n",
    "\n",
    "**8. How do you use MLflow in Databricks?**  \n",
    "\n",
    "**MLflow** is an open-source tool for:\n",
    "\n",
    "- **Tracking** ML experiments (metrics, parameters)\n",
    "- **Packaging** models\n",
    "- **Serving** models\n",
    "- Built-in with Databricks\n",
    "\n",
    "Example:\n",
    "```python\n",
    "import mlflow\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"alpha\", 0.5)\n",
    "    mlflow.log_metric(\"rmse\", 0.9)\n",
    "```\n",
    "\n",
    "You can view runs in **MLflow UI** or export models.\n",
    "\n",
    "---\n",
    "\n",
    "**9. What is Apache Arrow and how does it improve performance?**  \n",
    "\n",
    "**Apache Arrow** is a **columnar in-memory format** for efficient data exchange.\n",
    "\n",
    "- Improves performance between **Pandas ↔ PySpark**, or **Spark ↔ JVM ↔ Python**.\n",
    "- Enables **zero-copy reads**, reducing serialization cost.\n",
    "- Enable it in PySpark using:\n",
    "  ```python\n",
    "  spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "**10. How do you integrate Spark with Snowflake or BigQuery?**  \n",
    "\n",
    "Spark can read/write to Snowflake and BigQuery using connectors:\n",
    "\n",
    "- **Snowflake Spark Connector**: Use JDBC and native connector:\n",
    "  ```python\n",
    "  df.write.format(\"snowflake\").option(...).save()\n",
    "  ```\n",
    "- **BigQuery Connector for Spark**: Use Google-provided JARs to read/write tables:\n",
    "  ```python\n",
    "  df.write.format(\"bigquery\").option(\"table\", \"my_dataset.my_table\").save()\n",
    "  ```\n",
    "\n",
    "Both allow Spark to **offload computation** or **integrate data pipelines** with cloud data warehouses.\n",
    "\n",
    "---\n",
    "\n",
    "**11. How do you implement CI/CD pipelines for Databricks?**  \n",
    "\n",
    "CI/CD in Databricks includes:\n",
    "\n",
    "- **Source control**: Use GitHub, Azure Repos, GitLab with **Databricks Repos**.\n",
    "- **Build pipelines**:\n",
    "  - Test notebooks via `pytest`, `dbx`, or REST API\n",
    "  - Use **Databricks CLI** or `databricks-connect` for deployment\n",
    "- **Release pipelines**:\n",
    "  - Deploy notebooks, libraries, and jobs to dev/test/prod\n",
    "  - Use **Azure DevOps Pipelines** or **GitHub Actions**\n",
    "\n",
    "Supports **automated validation**, **notebook testing**, and **multi-environment deployments**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce8b8b4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
